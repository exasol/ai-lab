{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e31a8e-3da8-416f-9fc5-13d1c23664c1",
   "metadata": {},
   "source": [
    "# Generic Prediction UDF\n",
    "\n",
    "In this notebook, we are going to create a universal UDF SET script that will use a trained scikit-learn model for making a prediction. In order to use this script one has to create and train a scikit-learn model or a pipeline and upload its pickle file into the BucketFS. The list of features supplied to the UDF should match those used in model training and/or pre-processing. The script emits the prediction labels. The output of the script can be multi-dimensional. It works similarly in regression and classification scenarios.\n",
    "\n",
    "To communicate with the Exasol database we will be using the <a href=\"https://github.com/exasol/pyexasol\" target=\"_blank\" rel=\"noopener\">`pyexasol`</a> module.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Prior to using this notebook the following steps need to be completed:\n",
    "1. [Configure the sandbox](../sandbox_config.ipynb).\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Access configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca67bb9a-cb0d-4703-bfb3-6536b39be800",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/access_store_ui.ipynb\n",
    "display(get_access_store_ui('../'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f089728-cc11-4a3a-a7bf-f5a49e53a856",
   "metadata": {},
   "source": [
    "## Create UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c74dbb1-e40b-427e-9140-fc33ded4798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from exasol.connections import open_pyexasol_connection\n",
    "from stopwatch import Stopwatch\n",
    "\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Create script to test the model\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "CREATE OR REPLACE PYTHON3 SET SCRIPT\n",
    "{schema!q}.SKLEARN_PREDICT(...)\n",
    "EMITS(...) AS\n",
    "\n",
    "# Generic scikit-learn predictor that runs a prediction for a data batch.\n",
    "# Loads a scikit-learn model or a pipeline from the specified file. Calls its `predict` method\n",
    "# passing to it all provided data columns. Emits sample IDs and the output of the model.\n",
    "#\n",
    "# Note that the model should not include features' names!\n",
    "# \n",
    "# Input columns:\n",
    "#    [0]:  Full BucketFS path to the model file;\n",
    "#    [1]:  Sample ID, can be the ROWID of the test batch.\n",
    "#    [2+]: Feature columns.\n",
    "#\n",
    "# Output columns:\n",
    "#    [0]:  Sample ID copied from the input.\n",
    "#    [1+]: Model output.\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def run(ctx):\n",
    "    # Load model from EXABucket\n",
    "    with open(ctx[0], 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    # Stream the data through the model to reduce the required main memory of the UDF.\n",
    "    # This allows running the UDF on larger datasets.\n",
    "    while True:\n",
    "        # Read the input skipping the first column which holds the model path.\n",
    "        X_pred = ctx.get_dataframe(num_rows=1000, start_col=1)\n",
    "        if X_pred is None:\n",
    "            break\n",
    "\n",
    "        # Call the model to get the predictions. Omit the first column in the input\n",
    "        # which holds the sample IDs.\n",
    "        df_features = X_pred.drop(X_pred.columns[0], axis=1)\n",
    "        y_pred = model.predict(df_features)\n",
    "\n",
    "        # Combine predictions with the sample IDs.\n",
    "        df_rowid = X_pred[X_pred.columns[0]].reset_index(drop=True)\n",
    "        df_pred = pd.concat((df_rowid, pd.DataFrame(y_pred)), axis=1)\n",
    "\n",
    "        # Output data\n",
    "        ctx.emit(df_pred)\n",
    "/\n",
    "\"\"\")\n",
    "\n",
    "with open_pyexasol_connection(sb_config, compression=True) as conn:\n",
    "    conn.execute(query=sql, query_params={'schema': sb_config.SCHEMA})\n",
    "\n",
    "print(f\"Creating prediction script took: {stopwatch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eac55c-52c1-41e1-8df4-41958d994bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
