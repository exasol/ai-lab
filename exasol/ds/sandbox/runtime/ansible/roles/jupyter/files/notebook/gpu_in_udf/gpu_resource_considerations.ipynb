{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe069e74-f612-436c-8ad8-f4fc715a3594",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/exasol/ai-lab/refs/heads/main/assets/Exasol_Logo_2025_Dark.svg\" style=\"width:200px; margin: 10px;\" />\n",
    "</div>\n",
    "\n",
    "# GPU Resource Considerations\n",
    "\n",
    "In this tutorial we will learn about how to manage access to GPU's from UDF's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5843cc4-bfa2-4b7f-a5c1-184b5252daf7",
   "metadata": {},
   "source": [
    "## Open Secure Configuration Storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611eb3c5-2eec-4d22-8d11-227c4d9373e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%run ../utils/access_store_ui.ipynb\n",
    "display(get_access_store_ui('../'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c7b9a-00b1-401c-a2a8-a3a71ad3c297",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "- Run the [S3 VS tutorial](../cloud/02_s3_vs_reuters.ipynb) in order to populate the input data\n",
    "- Run the [Advanced UDF with GPU Support](./advanced_udf_with_gpu.ipynb) in order to populate the ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c907ff-dbb9-4487-8855-33dac6f858f1",
   "metadata": {},
   "source": [
    "### Instantiate ScriptLanguagesContainer\n",
    "\n",
    "The following cell creates an instance of class `ScriptLanguageContainer` from the notebook-connector,\n",
    "which enables us to use the Script Language Container (SLC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53d1b4-a568-47d3-8478-0aa1c3ee9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exasol.nb_connector.slc import ScriptLanguageContainer\n",
    "slc = ScriptLanguageContainer(secrets=ai_lab_config, name=\"gpu_slc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94bb0a-83c7-43f5-b223-fc1b45d4ecc3",
   "metadata": {},
   "source": [
    "### Import Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04513679-8f18-4bcc-9f97-69cd2bd064dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exasol.nb_connector.connections import open_pyexasol_connection\n",
    "from exasol.nb_connector.language_container_activation import open_pyexasol_connection_with_lang_definitions\n",
    "import textwrap\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3bd185-dcdc-4486-95e8-4f163cdf552b",
   "metadata": {},
   "source": [
    "### Check for Database compatibility\n",
    "\n",
    "This notebooks works only correct on an Exasol Database version 2025.2.0 or later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d34b111-049f-4853-ba64-7e5661a1610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging.version import Version\n",
    "with open_pyexasol_connection(ai_lab_config, compression=True, schema=ai_lab_config.db_schema) as conn:\n",
    "    res = conn.execute(\"SELECT PARAM_VALUE FROM EXA_METADATA WHERE PARAM_NAME='databaseProductVersion';\").fetchall();\n",
    "dbVersion = Version(res[0][0])\n",
    "if dbVersion < Version(\"2025.2.0\"):\n",
    "    popup_message(f\"This tutorial will not work correctly with the used database: The used queries here will not show the correct result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b474f9-4df5-4da6-9d5e-6b202fba4a69",
   "metadata": {},
   "source": [
    "### How GPU resources are managed\n",
    "\n",
    "In each select/sub-select statement, only a single UDF call is allowed to exclusively use all the available GPUs. The resource reservation by the GPU resource management happens per UDF call and individually for each UDF call.\n",
    "\n",
    "When multiple queries that each contain a single GPU-accelerated UDF are executed concurrently, the executions (UDF calls) are serialized. Each UDF call will wait to execute until an exclusive resource usage is made possible by accelerator resources being freed up by other UDF calls.\n",
    "\n",
    "This means that simultaneous execution of multiple GPU-accelerated UDF calls as part of a single select/sub-select statement is not possible. All UDF calls except one will in this case either fail or fall back to CPU usage, depending on the configuration.\n",
    "\n",
    "To simulate two simultaneous SQL queries, we create first a long running UDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73baccae-c17c-44fd-bf8e-9d8141e84fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "CREATE OR REPLACE {slc.language_alias} SCALAR SCRIPT\n",
    "long_running_gpu(id INTEGER)\n",
    "EMITS (ID INT, START_TIME VARCHAR(1000), END_TIME VARCHAR(1000), NVIDIA_VISIBLE_DEVICES VARCHAR(10000)) AS\n",
    " %perInstanceRequiredAcceleratorDevices GpuNvidia|None;\n",
    "\n",
    "import time\n",
    "\n",
    "def timestamp():\n",
    "    now = datetime.datetime.now().time()\n",
    "    return str(now)\n",
    "    \n",
    "def run(ctx):\n",
    "    start_timestamp = timestamp() \n",
    "    time.sleep(5)\n",
    "    nvidia_vis_devices = os.getenv('NVIDIA_VISIBLE_DEVICES', \"<invalid>\")\n",
    "    end_timestamp = timestamp()\n",
    "    ctx.emit(ctx.id, start_timestamp, end_timestamp,  nvidia_vis_devices)\n",
    "/\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection_with_lang_definitions(ai_lab_config, compression=True, schema=ai_lab_config.db_schema) as connection:\n",
    "    connection.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a66df-ec7a-40a0-b0f1-a2c53f792cf4",
   "metadata": {},
   "source": [
    "Now we execute this UDF in 2 different pyexasol connections, which run in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d674198-2263-42bb-a4a5-b19cbbf91e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def run_long_running_udf(id):\n",
    "    with open_pyexasol_connection_with_lang_definitions(ai_lab_config, schema=ai_lab_config.db_schema, compression=True) as local_conn:\n",
    "        sql = f\"SELECT long_running_gpu({id});\"\n",
    "        result = local_conn.execute(sql)\n",
    "        df = pd.DataFrame(result.fetchall(), columns=result.column_names())\n",
    "        return df\n",
    "\n",
    "with multiprocessing.Pool(processes=2) as pool:\n",
    "    results = pool.map(run_long_running_udf, [1, 2])\n",
    "\n",
    "res = pd.concat(results, ignore_index=True)\n",
    "res.sort_values(by='START_TIME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7c224-96d8-4425-8f57-ea5c83b91f7d",
   "metadata": {},
   "source": [
    "As you can see in the result above, the UDFs were executed sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5814a2-f60d-4e0a-8572-3c12eb454e8f",
   "metadata": {},
   "source": [
    "### Multiple GPU accelerated UDF calls in single query \n",
    "\n",
    "Using more than one GPU accelerated UDF call in a single query will either raise an error or fall back to CPU usage.\n",
    "\n",
    "Let's define a UDF which requires a GPU and then use the UDF twice in the same query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9420a-a21e-465a-aa71-5b7401245e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = textwrap.dedent(f\"\"\"\n",
    "CREATE OR REPLACE {slc.language_alias} SCALAR SCRIPT\n",
    "gpu_required()\n",
    "RETURNS VARCHAR(10000) AS\n",
    " %perInstanceRequiredAcceleratorDevices GpuNvidia;\n",
    "\n",
    "def run(ctx):\n",
    "    return os.getenv('NVIDIA_VISIBLE_DEVICES', \"<invalid>\")\n",
    "/\n",
    "\"\"\")\n",
    "with open_pyexasol_connection_with_lang_definitions(ai_lab_config, schema=ai_lab_config.db_schema, compression=True) as conn:\n",
    "    conn.execute(sql)\n",
    "\n",
    "try:\n",
    "    with open_pyexasol_connection_with_lang_definitions(ai_lab_config, schema=ai_lab_config.db_schema, compression=True) as conn:\n",
    "        stmt = conn.execute(\"SELECT gpu_required(), gpu_required();\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b772b4-1b2a-4d80-b60f-20c08ecb45bd",
   "metadata": {},
   "source": [
    "The same problem can happen with only one UDF which requires an GPU, and other UDFs with CPU fallback, as execution order is unpredictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a255b5cf-452c-4f8c-832a-42ec5ff48e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = textwrap.dedent(f\"\"\"\n",
    "CREATE OR REPLACE {slc.language_alias} SCALAR SCRIPT\n",
    "gpu_optional()\n",
    "RETURNS VARCHAR(10000) AS\n",
    " %perInstanceRequiredAcceleratorDevices GpuNvidia|None;\n",
    "\n",
    "def run(ctx):\n",
    "    return os.getenv('NVIDIA_VISIBLE_DEVICES', \"<invalid>\")\n",
    "/\n",
    "\"\"\")\n",
    "\n",
    "with open_pyexasol_connection_with_lang_definitions(ai_lab_config, schema=ai_lab_config.db_schema, compression=True) as conn:\n",
    "    conn.execute(sql)\n",
    "\n",
    "try:\n",
    "    with open_pyexasol_connection_with_lang_definitions(ai_lab_config, schema=ai_lab_config.db_schema, compression=True) as conn:\n",
    "        stmt = conn.execute(\"SELECT gpu_required(), gpu_required();\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe5be5-1d75-43ce-9c68-b03aba890d36",
   "metadata": {},
   "source": [
    "If your query contains only UDFs that support CPU fallback, one of the UDFs will be assigned the GPU accelerator. However, it is not possible to predict which one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93efe9d1-e8eb-4790-a42d-a5fc965484cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_pyexasol_connection_with_lang_definitions(ai_lab_config, schema=ai_lab_config.db_schema, compression=True) as conn:\n",
    "    res = conn.export_to_pandas(\"SELECT gpu_optional() as udf1, gpu_optional() as udf2, gpu_optional() as udf3, gpu_optional() as udf4;\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2ce16-330a-464b-813a-42498ba91bb8",
   "metadata": {},
   "source": [
    "### GPU usage across UDF instances\n",
    "\n",
    "If there are multiple UDF instances of a single UDF call within the same query, all those instances will have access to all (GPU) accelerators. To reduce the risk of running out of GPU memory, we recommend that you use UDF instance limiting to control how many accelerators each instance can use, and that you configure the instance limit based on the number of accelerators and the use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa6eca-77d5-4421-879e-df6c97b4c5c1",
   "metadata": {},
   "source": [
    "#### Scalar UDFs\n",
    "For Scalar UDFs there is no mechanism to assign UDF to a certain GPU. Hence, we recommend to use Scalar UDF only to read some GPU device information, but do not any processing on the GPUs. A good example is to execute the `nvidia-smi` tool in a scalar UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8d3cf9-afc9-4165-a12e-7ba9f97fb061",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = textwrap.dedent(f\"\"\"\n",
    "CREATE OR REPLACE {slc.language_alias} SCALAR SCRIPT\n",
    "gpu_nvidia_smi()\n",
    "RETURNS VARCHAR(10000) AS\n",
    " %perInstanceRequiredAcceleratorDevices GpuNvidia;\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def run(ctx):\n",
    "    cmd = [\"nvidia-smi\"] #List GPU's\n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "    )\n",
    "    exit_code = process.wait()\n",
    "    \n",
    "    if exit_code != 0:\n",
    "        raise Exception(f\"nvidia-smi returned non-zero exit code: '{{process.stderr}}'\")\n",
    "    return process.stdout.read()\n",
    "\n",
    "/\n",
    "\"\"\")\n",
    "with open_pyexasol_connection_with_lang_definitions(ai_lab_config, schema=ai_lab_config.db_schema, compression=True) as conn:\n",
    "    conn.execute(sql)\n",
    "\n",
    "with open_pyexasol_connection_with_lang_definitions(ai_lab_config, schema=ai_lab_config.db_schema, compression=True) as conn:\n",
    "    res = conn.execute(\"SELECT gpu_nvidia_smi();\").fetchall()\n",
    "print(res[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4feae66-c0a6-4418-a048-107991cc9231",
   "metadata": {},
   "source": [
    "#### Set UDFs\n",
    "\n",
    "For set UDF it is possible to generate a group per GPU and pass this `GPU id` to the UDF as parameter.\n",
    "\n",
    "For this purpose we modify the example from the [Advanced UDF with GPU Support](./advanced_udf_with_gpu.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c198ad-4300-4a30-a018-fada3a447f41",
   "metadata": {},
   "source": [
    "First, we need the number of available GPUs. This can be achieved by reading the `EXA_METADATA` parameter `acceleratorDeviceGpuNvidiaCount`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0fa92c-6738-4e2d-9c98-8151a04b30cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_pyexasol_connection_with_lang_definitions(ai_lab_config, schema=ai_lab_config.db_schema, compression=True) as conn:\n",
    "    nAvailableGPU = conn.execute(\"SELECT PARAM_VALUE FROM EXA_METADATA WHERE PARAM_NAME='acceleratorDeviceGpuNvidiaCount'\").fetchall()[0][0]\n",
    "nAvailableGPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8dfae-30cf-4e41-abe5-806de9185149",
   "metadata": {},
   "source": [
    "Now need modify the \"SEMANTIC_JOIN\" UDF and add a GPU id parameter:\n",
    "```\n",
    "CREATE OR REPLACE {slc.language_alias} SET SCRIPT \"SEMANTIC_JOIN\"(gpu_id INTEGER, text1 VARCHAR(2000000), text2 VARCHAR(2000000))\n",
    "```\n",
    "\n",
    "Also, we set the UDF instance limit to the number of available GPUs, to avoid parallel access from differnt UDF instances to the same GPU.\n",
    "```\n",
    "%perNodeAndCallInstanceLimit {{nAvailableGPU}};\n",
    "```\n",
    "\n",
    "Finally, we use the given GPU id from the UDF call and specify it when we instantiate the `pytorch` device:\n",
    "```\n",
    "gpu_id = df[\"gpu_id\"][0] #Just take the first one. The caller needs to ensure that column gpu_id has only identical values\n",
    "device = torch.device(f\"cuda:{{gpu_id}}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75802b-ef66-4341-906a-18009310e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_semantic_join = f\"\"\"\n",
    "--/\n",
    "CREATE OR REPLACE {slc.language_alias} SET SCRIPT \"SEMANTIC_JOIN\"(gpu_id INTEGER, text1 VARCHAR(2000000), text2 VARCHAR(2000000))\n",
    "EMITS(gpu_id INTEGER, text1 VARCHAR(2000000), text2 VARCHAR(2000000), similarity_score DOUBLE) AS\n",
    "%perInstanceRequiredAcceleratorDevices GpuNvidia;\n",
    "%perNodeAndCallInstanceLimit {nAvailableGPU};\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, Pipeline, pipeline\n",
    "import torch\n",
    "import exasol.bucketfs as bfs\n",
    "from pathlib import Path\n",
    "from exasol_transformers_extension.utils import device_management\n",
    "from exasol.python_extension_common.connections.bucketfs_location import (\n",
    "    create_bucketfs_location_from_conn_object)\n",
    "from exasol_transformers_extension.utils.bucketfs_model_specification import (\n",
    "    BucketFSModelSpecification)\n",
    "from exasol_transformers_extension.utils.huggingface_hub_bucketfs_model_transfer_sp import (\n",
    "    HuggingFaceHubBucketFSModelTransferSP)\n",
    "from exasol_transformers_extension.utils.load_local_model import LoadLocalModel\n",
    "\n",
    "\n",
    "def get_bucketfs_location(exa, bucketfs_conn_name: str) -> bfs.path.PathLike:\n",
    "    return create_bucketfs_location_from_conn_object(\n",
    "        exa.get_connection(bucketfs_conn_name))\n",
    "\n",
    "def load_transformers_pipline(exa,\n",
    "                              bucketfs_conn_name: str,\n",
    "                              sub_dir: str,\n",
    "                              device: str,\n",
    "                              task_type: str,\n",
    "                              model_name: str,\n",
    "                              model_factory,\n",
    "                              tokenizer_factory=AutoTokenizer) -> Pipeline:\n",
    "    model_loader = LoadLocalModel(pipeline,\n",
    "                                  base_model_factory=model_factory,\n",
    "                                  tokenizer_factory=tokenizer_factory,  # type: ignore\n",
    "                                  task_type=task_type,\n",
    "                                  device=device)    # type: ignore\n",
    "\n",
    "    model_spec = BucketFSModelSpecification(model_name, task_type, bucketfs_conn_name,\n",
    "                                            Path(sub_dir))\n",
    "\n",
    "    bucketfs_location = get_bucketfs_location(exa, bucketfs_conn_name)\n",
    "\n",
    "    model_loader.clear_device_memory()\n",
    "    model_loader.set_current_model_specification(model_spec)\n",
    "    model_loader.set_bucketfs_model_cache_dir(bucketfs_location)\n",
    "    return model_loader.load_models()\n",
    "\n",
    "# Function to calculate embeddings\n",
    "def batch_get_embeddings(batch_texts, model_pipeline, device):\n",
    "    # Tokenize the batch of texts\n",
    "    inputs = model_pipeline.tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt').to(device)  # Send to GPU\n",
    "    # Get embeddings from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model_pipeline.model(**inputs)\n",
    "    # Use the outputs pooler_output or last_hidden_state to get the embedding\n",
    "    return outputs.pooler_output if 'pooler_output' in outputs else outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Function to compute row-by-row cosine similarity using PyTorch\n",
    "def row_by_row_cosine_similarity_gpu(embeddings1, embeddings2):\n",
    "    # Normalized embeddings can be calculated separately\n",
    "    normalized_embeddings1 = embeddings1 / embeddings1.norm(dim=1)[:, None]\n",
    "    normalized_embeddings2 = embeddings2 / embeddings2.norm(dim=1)[:, None]\n",
    "\n",
    "    # Compute pair-wise cosine similarity for each corresponding pair\n",
    "    similarities = (normalized_embeddings1 * normalized_embeddings2).sum(dim=1)\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def get_model_pipeline(device):\n",
    "    return load_transformers_pipline(\n",
    "        exa,\n",
    "        bucketfs_conn_name='{ai_lab_config.bfs_connection_name}',\n",
    "        sub_dir='{ai_lab_config.bfs_model_subdir}',\n",
    "        device=device,\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        task_type='feature-extraction',\n",
    "        model_factory=AutoModel)\n",
    "\n",
    "def run(ctx):\n",
    "    \n",
    "    # Batch processing configurations\n",
    "    batch_size = 100  # Adjust this based on your memory capacity\n",
    "    model_pipeline = None\n",
    "    while True:\n",
    "        df = ctx.get_dataframe(batch_size)\n",
    "        if df is None:\n",
    "            break\n",
    "\n",
    "        data1 = df[\"text1\"]\n",
    "        data2 = df[\"text2\"]\n",
    "        gpu_id = df[\"gpu_id\"][0] #Just take the first one. The caller needs to ensure that column gpu_id has only identical values\n",
    "        device = torch.device(f\"cuda:{{gpu_id}}\")\n",
    "        if model_pipeline is None:\n",
    "            model_pipeline = get_model_pipeline(device)\n",
    "\n",
    "        # Compute embeddings for each dataset in batches\n",
    "        embeddings1 = batch_get_embeddings(data1.to_list(), model_pipeline, device)\n",
    "        embeddings2 = batch_get_embeddings(data2.to_list(), model_pipeline, device)\n",
    "\n",
    "        # Calculate row-by-row cosine similarity using PyTorch on GPU\n",
    "        similarity_scores = row_by_row_cosine_similarity_gpu(embeddings1, embeddings2)\n",
    "\n",
    "        # Determine matches based on similarity scores (threshold can be adjusted)\n",
    "        df[\"SIMILARITY_SCORE\"]=similarity_scores.cpu()\n",
    "        ctx.emit(df)\n",
    "/\n",
    "\"\"\"\n",
    "with open_pyexasol_connection_with_lang_definitions(ai_lab_config, schema=ai_lab_config.db_schema, compression=True) as conn:\n",
    "    conn.execute(sql_semantic_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ec02a-fea9-4efd-ad10-bff3b54aa49d",
   "metadata": {},
   "source": [
    "Finally, we modify the SQL query from the Semantic Join query, and group the data by GPU id and Exasol node.\n",
    "For this purpose we simply create an integer value between 0 and the number of available GPUs and assign this to each row with:\n",
    "```\n",
    "MOD(ROW_NUMBER() OVER (), {nAvailableGPU}) AS GPU_ID\n",
    "```\n",
    "Then we add this integer value to the UDF call:\n",
    "```\n",
    "\"SEMANTIC_JOIN\"(GPU_ID, TITLE, TOPIC)\n",
    "```\n",
    "The data we use to call the UDF looks then like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f6bcd-9dd3-4566-8324-8f0f11788ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = textwrap.dedent(f\"\"\"\n",
    "SELECT \n",
    "    MOD(ROW_NUMBER() OVER (), {nAvailableGPU}) AS GPU_ID, TOPIC, TITLE\n",
    "FROM (\n",
    "    SELECT \n",
    "        distinct TOPIC, TITLE\n",
    "    FROM (\n",
    "        SELECT distinct title as TITLE FROM NEWS WHERE TITLE IS NOT NULL AND BODY IS NOT NULL\n",
    "    )\n",
    "    CROSS JOIN (\n",
    "        SELECT distinct topic AS TOPIC FROM NEWS WHERE topic IS NOT NULL\n",
    "    )\n",
    ")\n",
    "\"\"\")\n",
    "with open_pyexasol_connection_with_lang_definitions(ai_lab_config, schema=ai_lab_config.db_schema, compression=True) as conn:\n",
    "    res = conn.export_to_pandas(sql)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208fa0b7-a737-419c-ae98-541cfc891dfe",
   "metadata": {},
   "source": [
    "Finally we call the UDF to calculate the semantic join, distributed safely across multiple GPUs and nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9886b4638692671",
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql1"
    }
   },
   "outputs": [],
   "source": [
    "sql_semantic_join = textwrap.dedent(f\"\"\"\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT\n",
    "        TEXT1 AS TITLE, TEXT2 AS TOPIC,\n",
    "        RANK() OVER (PARTITION BY TEXT1 ORDER BY SIMILARITY_SCORE DESC) as r,\n",
    "        SIMILARITY_SCORE\n",
    "    FROM (\n",
    "        SELECT\n",
    "            \"SEMANTIC_JOIN\"(GPU_ID, TITLE, TOPIC)\n",
    "        FROM (\n",
    "            SELECT \n",
    "                MOD(ROW_NUMBER() OVER (), {nAvailableGPU}) AS GPU_ID, TOPIC, TITLE\n",
    "            FROM (\n",
    "                SELECT \n",
    "                    distinct TOPIC, TITLE\n",
    "                FROM (\n",
    "                    SELECT distinct title as TITLE FROM NEWS WHERE TITLE IS NOT NULL AND BODY IS NOT NULL\n",
    "                )\n",
    "                CROSS JOIN (\n",
    "                    SELECT distinct topic AS TOPIC FROM NEWS WHERE topic IS NOT NULL\n",
    "                )\n",
    "            )\n",
    "        ) GROUP BY iproc(), GPU_ID\n",
    "    )\n",
    "    WHERE SIMILARITY_SCORE > 0.5\n",
    ")\n",
    "WHERE r<=5\n",
    "ORDER BY TITLE, r ASC;\n",
    "\"\"\")\n",
    "with open_pyexasol_connection_with_lang_definitions(ai_lab_config, schema=ai_lab_config.db_schema, compression=True) as conn:\n",
    "    res = conn.export_to_pandas(sql_semantic_join)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a29a09-705e-4530-b66a-7d8fdabfcf9d",
   "metadata": {},
   "source": [
    "The following is a more generic approach, with which you can assign UDF instances to GPU's, on multiple nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c46968-1be9-43d8-8885-3f30a48d7ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
