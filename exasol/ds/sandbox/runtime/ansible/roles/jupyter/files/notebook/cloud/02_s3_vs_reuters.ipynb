{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this notebook we create a Virtual Schema which accesses Json files on a public S3 bucket.\n",
    "The data we're loading is a part of [Reuter 21578](https://paperswithcode.com/dataset/reuters-21578) dataset, which is a collection of documents with news articles. "
   ],
   "id": "79790cc96f0d5bb2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "%run ../utils/access_store_ui.ipynb\n",
    "display(get_access_store_ui('../'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As a first step we setting up the S3 Virtual Schema connector in the database.\n",
    "You can find more information in the [s3-document-files-virtual-schema](https://github.com/exasol/s3-document-files-virtual-schema/) GitHub repo. "
   ],
   "id": "6c0f779ad4690794"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%run ./s3_vs_setup.ipynb",
   "id": "275dd63eca698b42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create Virtual Schema\n",
    "\n",
    "To configure the virtual schema, we need the following:\n",
    "\n",
    "1. Create an S3 Connection object: in what S3 bucket our data resides. Our data set is on the public S3 bucket, so no S3 credentials are needed. If your data is not open to the public, you'll need credentials to be stored in the Connection object.\n",
    "2. Put EDML file into teh Bucket FS. EDML describes the mapping between Json structure and desired Virtual Schema tables and columns. In our case the mapping is simple, as every json file describes individual news article with the same set of columns. But EDML supports more complicated scenarios, like table references, foreign keys and missing columns. You can find more details [in the documentation](https://github.com/exasol/virtual-schema-common-document/blob/main/doc/user_guide/edml_user_guide.md)\n",
    "3. Create Virtual Schema with S3 Connection object and EDML file in Bucket FS. After this step, new Virtual Schema tables are added to the Exasol, and every query from them performs S3 data read."
   ],
   "id": "2b83bf8534f1ee88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pyexasol\n",
    "import pathlib\n",
    "from exasol.nb_connector.connections import open_pyexasol_connection\n",
    "from exasol.nb_connector import bfs_utils"
   ],
   "id": "4746ccef6c0fed1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sql = \"\"\"\n",
    "CREATE CONNECTION S3_CONNECTION_VS\n",
    "  TO ''\n",
    "  USER ''\n",
    "  IDENTIFIED BY '{\n",
    "      \"awsRegion\": \"eu-central-1\", \n",
    "      \"s3Bucket\": \"ai-lab-example-data-s3\" \n",
    "  }';\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    conn.execute(sql)"
   ],
   "id": "47c55fee36abfb34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Our EDML extracts 3 fields from json files:\n",
    "* `title`: as a varchar\n",
    "* `id` (named as `NEWS_ID`): article identifier stored as a string\n",
    "* `body`: text of the article up to 1024 characters long\n",
    "\n",
    "In addition to those 3 fields, we have field `topics`, which is extracted as a reference table.\n",
    "In the json data topics are represented as a list of tags associated with the article, for example:\n",
    "\n",
    "```\n",
    "    \"topics\": [\n",
    "      \"grain\",\n",
    "      \"wheat\",\n",
    "      \"corn\",\n",
    "      \"barley\",\n",
    "      \"oat\",\n",
    "      \"sorghum\"\n",
    "    ],\n",
    "```\n",
    "\n",
    "To have convenient access to topics, we use `toTableMapping` EDML feature, which creates a second table `NEWS_TOPICS`, keeping list of topics and `NEWS_ID`:\n",
    "\n",
    "```\n",
    "      \"topics\": {\n",
    "        \"toTableMapping\": {\n",
    "          \"mapping\": {\n",
    "            \"toVarcharMapping\": {\n",
    "              \"destinationName\": \"NAME\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "```"
   ],
   "id": "5d450a20e2ce88b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# put edml to bucket fs\n",
    "bfs_bucket = open_bucketfs_connection(ai_lab_config)\n",
    "bfs_path = bfs_utils.put_file(bfs_bucket, pathlib.Path(\"reuters-edml.json\"))"
   ],
   "id": "e46e42baa2a61831"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "bfs_path.as_udf_path()",
   "id": "f0af003ba35b9ff0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sql = \"\"\"\n",
    "CREATE VIRTUAL SCHEMA {schema_name!i}_VS USING {schema_name!i}.S3_FILES_ADAPTER WITH\n",
    "    CONNECTION_NAME = 'S3_CONNECTION_VS'\n",
    "    MAPPING         = {map_file!s};\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    conn.execute(sql, query_params={\n",
    "        \"schema_name\": ai_lab_config.db_schema,\n",
    "        \"map_file\": bfs_path.as_udf_path()\n",
    "    })"
   ],
   "id": "e11cb124291c57ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Query the data\n",
    "\n",
    "Once we have our Virtual Schema created, new table `AI_LAB_VS.NEWS` is available for query. \n",
    "But you need to be aware of performance overhead. Virtal Schemas don't do any intermediate caching and every request to the Virtual Schema table performs several S3 data reads, which might be slow.\n",
    "\n",
    "If you're going to access the data several times, it might make sense to create the new table and copy the data from VS table in a normal table. Here we don't do that for simplicity - we do just one query and that's fine. "
   ],
   "id": "ee0b18f1ed03d5a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sql = \"\"\"\n",
    "select title, news_id from {schema_name!i}_VS.NEWS\n",
    "limit 10\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    res = conn.execute(sql, query_params={\n",
    "        \"schema_name\": ai_lab_config.db_schema\n",
    "    })\n",
    "    for r in res:\n",
    "        print(r)"
   ],
   "id": "19c4e5922c9b17d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Besides the `NEWS` table, where each row is a news article, we have reference table with topics associated with every article.\n",
    "In our EDML file, `toTableMapping` was used, which created a second table `NEWS_TOPICS`, containing flattened list of topics associated with `NEWS_ID`."
   ],
   "id": "fb187115aa035660"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sql = \"\"\"\n",
    "select *\n",
    "from {schema_name!i}_VS.NEWS_TOPICS\n",
    "limit 10\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    res = conn.execute(sql, query_params={\n",
    "        \"schema_name\": ai_lab_config.db_schema\n",
    "    })\n",
    "    for r in res:\n",
    "        print(r)"
   ],
   "id": "5b7258505e2a0a3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So, we can can get most frequent topics for our dataset, for example",
   "id": "c583bab31ea654cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sql = \"\"\"\n",
    "select NAME, count(*)\n",
    "from {schema_name!i}_VS.NEWS_TOPICS\n",
    "group by 1\n",
    "order by 2 desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    res = conn.execute(sql, query_params={\n",
    "        \"schema_name\": ai_lab_config.db_schema\n",
    "    })\n",
    "    for r in res:\n",
    "        print(r)"
   ],
   "id": "e1f938fa095315c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Both tables `NEWS` and `NEWS_TOPICS` could be joined on `NEWS_ID` column:",
   "id": "8dd75dbbd2a7fd0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sql = \"\"\"\n",
    "select topics.name, news.title, news.news_id\n",
    "from \n",
    "    {schema_name!i}_VS.NEWS as news,\n",
    "    {schema_name!i}_VS.NEWS_TOPICS as topics\n",
    "where news.NEWS_ID = topics.NEWS_NEWS_ID\n",
    "\n",
    "limit 10\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    res = conn.execute(sql, query_params={\n",
    "        \"schema_name\": ai_lab_config.db_schema\n",
    "    })\n",
    "    for r in res:\n",
    "        print(r)"
   ],
   "id": "53757c4e163bc774"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
