{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this notebook we create a Virtual Schema which accesses JSON files on a public S3 bucket.\n",
    "The data we are loading is a part of the [Reuters 21578](https://paperswithcode.com/dataset/reuters-21578) dataset, which is a collection of documents with news articles. "
   ],
   "id": "79790cc96f0d5bb2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "%run ../utils/access_store_ui.ipynb\n",
    "display(get_access_store_ui('../'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As a first step, we are setting up the S3 Virtual Schema connector in the database.\n",
    "You can find more information in the [s3-document-files-virtual-schema](https://github.com/exasol/s3-document-files-virtual-schema/) GitHub repository. "
   ],
   "id": "6c0f779ad4690794"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%run ./s3_vs_setup.ipynb",
   "id": "275dd63eca698b42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create Virtual Schema\n",
    "\n",
    "To configure the virtual schema, we need the following:\n",
    "\n",
    "1. Create an S3 Connection object, which specifies in which S3 bucket our data resides. Our dataset is on a public S3 bucket, so no S3 credentials are needed. If your data is not open to the public, you need to store credentials in the Connection object.\n",
    "2. Put the EDML file into the Bucket FS. EDML describes the mapping between the JSON structure and the desired Virtual Schema tables and columns. In our case, the mapping is simple, as every JSON file describes an individual news article with the same set of columns. However, EDML supports more complex scenarios, like table references, foreign keys, and missing columns. You can find more details [in the documentation.](https://github.com/exasol/virtual-schema-common-document/blob/main/doc/user_guide/edml_user_guide.md)\n",
    "3. Create a Virtual Schema with the S3 Connection object and the EDML file in the Bucket FS. After this step, new virtual tables are added to Exasol, and every query from them performs S3 data read."
   ],
   "id": "2b83bf8534f1ee88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pyexasol\n",
    "import pathlib\n",
    "from exasol.nb_connector.connections import open_pyexasol_connection\n",
    "from exasol.nb_connector import bfs_utils"
   ],
   "id": "4746ccef6c0fed1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sql = \"\"\"\n",
    "CREATE CONNECTION S3_CONNECTION_VS\n",
    "  TO ''\n",
    "  USER ''\n",
    "  IDENTIFIED BY '{\n",
    "      \"awsRegion\": \"eu-central-1\", \n",
    "      \"s3Bucket\": \"ai-lab-example-data-s3\" \n",
    "  }';\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    conn.execute(sql)"
   ],
   "id": "47c55fee36abfb34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Our EDML definition extracts 3 fields from JSON files:\n",
    "* `title`: as a VARCHAR(255) where longer texts are truncated.\n",
    "* `id` (named as `NEWS_ID`): article identifier stored as a VARCHAR(50)\n",
    "* `body`: text of the article as VARCHAR(1024) where longer texts are truncated.\n",
    "\n",
    "In addition to those 3 fields, we have the field `topics`, which is extracted as a reference table.\n",
    "In the JSON data, topics are represented as a list of tags associated with the article, for example:\n",
    "\n",
    "```\n",
    "    \"topics\": [\n",
    "      \"grain\",\n",
    "      \"wheat\",\n",
    "      \"corn\",\n",
    "      \"barley\",\n",
    "      \"oat\",\n",
    "      \"sorghum\"\n",
    "    ],\n",
    "```\n",
    "\n",
    "To have convenient access to the topics, we use the `toTableMapping` EDML feature, which creates a second table `NEWS_TOPICS`, keeping a list of topics and `NEWS_ID`:\n",
    "\n",
    "```\n",
    "      \"topics\": {\n",
    "        \"toTableMapping\": {\n",
    "          \"mapping\": {\n",
    "            \"toVarcharMapping\": {\n",
    "              \"destinationName\": \"NAME\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "```"
   ],
   "id": "5d450a20e2ce88b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# put edml to bucket fs\n",
    "bfs_bucket = open_bucketfs_connection(ai_lab_config)\n",
    "bfs_path = bfs_utils.put_file(bfs_bucket, pathlib.Path(\"reuters-edml.json\"))"
   ],
   "id": "e46e42baa2a61831"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "bfs_path.as_udf_path()",
   "id": "f0af003ba35b9ff0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sql = \"\"\"\n",
    "CREATE VIRTUAL SCHEMA {schema_name!i}_VS USING {schema_name!i}.S3_FILES_ADAPTER WITH\n",
    "    CONNECTION_NAME = 'S3_CONNECTION_VS'\n",
    "    MAPPING         = {map_file!s};\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    conn.execute(sql, query_params={\n",
    "        \"schema_name\": ai_lab_config.db_schema,\n",
    "        \"map_file\": bfs_path.as_udf_path()\n",
    "    })"
   ],
   "id": "e11cb124291c57ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Query the data\n",
    "\n",
    "Once we have our Virtual Schema created, a new table `AI_LAB_VS.NEWS` is available for querying. \n",
    "*Note*: you need to be aware of performance overhead. Virtual Schemas don't do any intermediate caching, and every request to a virtual table performs several S3 data reads, which might be slow.\n",
    "\n",
    "If you are going to access the data several times, it might make sense to create the new table and copy the data from the virtual table into a normal table. Here we don't do that for simplicity. "
   ],
   "id": "ee0b18f1ed03d5a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sql = \"\"\"\n",
    "select title, news_id from {schema_name!i}_VS.NEWS\n",
    "limit 10\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    res = conn.execute(sql, query_params={\n",
    "        \"schema_name\": ai_lab_config.db_schema\n",
    "    })\n",
    "    for r in res:\n",
    "        print(r)"
   ],
   "id": "19c4e5922c9b17d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Besides the `NEWS` table, where each row is a news article, we additionally have a reference table with topics associated with every article.\n",
    "In our EDML file, `toTableMapping` was used, which created a second table `NEWS_TOPICS`, containing flattened list of topics associated with `NEWS_ID`."
   ],
   "id": "fb187115aa035660"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sql = \"\"\"\n",
    "select *\n",
    "from {schema_name!i}_VS.NEWS_TOPICS\n",
    "limit 10\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    res = conn.execute(sql, query_params={\n",
    "        \"schema_name\": ai_lab_config.db_schema\n",
    "    })\n",
    "    for r in res:\n",
    "        print(r)"
   ],
   "id": "5b7258505e2a0a3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So, the query below can be used to obtain the most frequent topics for our dataset.",
   "id": "c583bab31ea654cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sql = \"\"\"\n",
    "select NAME, count(*)\n",
    "from {schema_name!i}_VS.NEWS_TOPICS\n",
    "group by 1\n",
    "order by 2 desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    res = conn.execute(sql, query_params={\n",
    "        \"schema_name\": ai_lab_config.db_schema\n",
    "    })\n",
    "    for r in res:\n",
    "        print(r)"
   ],
   "id": "e1f938fa095315c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The tables `NEWS` and `NEWS_TOPICS` can also be joined on `NEWS_ID` column to find articles about a specific topic:",
   "id": "8dd75dbbd2a7fd0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sql = \"\"\"\n",
    "select topics.name, news.title, news.news_id\n",
    "from \n",
    "    {schema_name!i}_VS.NEWS as news,\n",
    "    {schema_name!i}_VS.NEWS_TOPICS as topics\n",
    "where news.NEWS_ID = topics.NEWS_NEWS_ID\n",
    "and topics.NAME = 'grain'\n",
    "\n",
    "limit 10\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    res = conn.execute(sql, query_params={\n",
    "        \"schema_name\": ai_lab_config.db_schema\n",
    "    })\n",
    "    for r in res:\n",
    "        print(r)"
   ],
   "id": "53757c4e163bc774"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Materialize the news data in a table. This speeds up processing of the data in other notebooks.",
   "id": "fe367cfb8f283940"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sql = \"\"\"\n",
    "CREATE TABLE {schema_name!i}.NEWS as select T.NAME as TOPIC, N.TITLE as TITLE, N.NEWS_ID as NEWS_ID, N.BODY as BODY\n",
    "FROM\n",
    "    {schema_name!i}_VS.NEWS as N,\n",
    "    {schema_name!i}_VS.NEWS_TOPICS as T\n",
    "WHERE news.NEWS_ID = topics.NEWS_NEWS_ID;\n",
    "\"\"\"\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    res = conn.execute(sql, query_params={\n",
    "        \"schema_name\": ai_lab_config.db_schema\n",
    "    })"
   ],
   "id": "602326d92e9d4f53"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
