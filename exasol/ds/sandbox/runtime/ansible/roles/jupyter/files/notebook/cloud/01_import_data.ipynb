{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importing data from cloud storage\n",
    "\n",
    "In this notebook we'll use [cloud-storage-extension](https://github.com/exasol/cloud-storage-extension/) to import publicly available data from AWS S3 into the Exasol database. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ea9d8e0ecc5bbd3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before importing the data we need to configure the database, by setting up cloud-storage-extension jar files and UDF scripts used for the import. This need to be done once for the database."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1912d75f3f8f8b87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Access configuration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40948e6b5e139445"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%run ../utils/access_store_ui.ipynb\n",
    "display(get_access_store_ui('../'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3e09393f1b792f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup of cloud-storage-extension"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63f58637e1c5c7a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%run ../cloud_store_config.ipynb"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95d09c1457b4ae0b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing small amount of data from parquet files\n",
    "\n",
    "For the beginning, we'll load small volume of data from publicly available [Youtube 8M dataset](https://registry.opendata.aws/yt8m/).\n",
    "\n",
    "In this example, we'll work with \"dataset vocabulary\" which is information about classes of videos. In total there are 3862 entries, which are stored in one single parquet file: \n",
    "s3://aws-roda-ml-datalake/yt8m_ods/vocabulary/run-1644252350398-part-block-0-r-00000-snappy.parquet\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "839a313d4d1c26d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pyexasol\n",
    "from exasol.connections import open_pyexasol_connection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6003fd367d36d89a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Schema and target table\n",
    "\n",
    "As a first step, we need to obtain the schema of the data (set of columns stored in parquet files with their types). You might have this information in advance (if this is your dataset), but if not, you need to analyze parquet files to figure out their schema.\n",
    "\n",
    "One of the options of doing this is the parquet-tools library wrapped into a [docker container](https://hub.docker.com/r/nathanhowell/parquet-tools). To use it, you need to download one of parquet files locally, then run this docker container against this file. Using the same container, you can also peek into parquet files and looks at its actual data.\n",
    "\n",
    "For the file above, I got the following schema information:\n",
    "\n",
    "```\n",
    "message glue_schema {\n",
    "  optional binary Index (STRING);\n",
    "  optional binary TrainVideoCount (STRING);\n",
    "  optional binary KnowledgeGraphId (STRING);\n",
    "  optional binary Name (STRING);\n",
    "  optional binary WikiUrl (STRING);\n",
    "  optional binary Vertical1 (STRING);\n",
    "  optional binary Vertical2 (STRING);\n",
    "  optional binary Vertical3 (STRING);\n",
    "  optional binary WikiDescription (STRING);\n",
    "}\n",
    "```  \n",
    "\n",
    "From this schema we see that all the columns in parquet file have string type and optional (nullable).\n",
    "Let's create the table in our database for this data. The names of columns are not important, just the order and their types have to match with parquet file schema."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dc968652856f86"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "TABLE_NAME = \"Y8M_CLASSES\"\n",
    "\n",
    "sql = \"\"\"\n",
    "create or replace table {schema_name!i}.{table_name!i} \n",
    "(\n",
    "    ClsIndex          VARCHAR2(1024),\n",
    "    TrainVideoCount   VARCHAR2(1024),\n",
    "    KnowledgeGraphId  VARCHAR2(1024),\n",
    "    Name              VARCHAR2(1024),\n",
    "    WikiUrl           VARCHAR2(1024),\n",
    "    Vertical1         VARCHAR2(1024),\n",
    "    Vertical2         VARCHAR2(1024),\n",
    "    Vertical3         VARCHAR2(1024),\n",
    "    WikiDescription   VARCHAR2(2048)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(sb_config) as conn:\n",
    "    conn.execute(sql, query_params={\n",
    "        \"schema_name\": sb_config.db_schema,\n",
    "        \"table_name\": TABLE_NAME\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T13:46:24.373333Z",
     "start_time": "2023-11-03T13:46:24.288010Z"
    }
   },
   "id": "21cfe3edd3246a51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## S3 credentials\n",
    "\n",
    "If S3 bucket is public, we can pass empty access key and secret keys. Otherwise replace with valid credentials."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df21776e2435eced"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "CREATE OR REPLACE CONNECTION S3_CONNECTION TO '' USER '' \n",
    "IDENTIFIED BY 'S3_ACCESS_KEY={access_key!r};S3_SECRET_KEY={secret_key!r}';\n",
    "\"\"\"\n",
    "\n",
    "S3_ACCESS_KEY = \"\"\n",
    "S3_SECRET_KEY = \"\"\n",
    "\n",
    "with open_pyexasol_connection(sb_config) as conn:\n",
    "    conn.execute(sql, query_params={\n",
    "        \"schema\": sb_config.db_schema,\n",
    "        \"access_key\": S3_ACCESS_KEY,\n",
    "        \"secret_key\": S3_SECRET_KEY,\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T13:43:05.595870Z",
     "start_time": "2023-11-03T13:43:05.551031Z"
    }
   },
   "id": "b2ec486162082706"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing data\n",
    "\n",
    "Now it's time to import our data. We call the `IMPORT_PATH` script, providing the location of parquet files, their format, the s3 endpoint (which has to match the bucket's configuration) and the name of our connection object."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50a05075716522ec"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"schema\": sb_config.db_schema,\n",
    "    \"table\": TABLE_NAME,  \n",
    "}\n",
    "\n",
    "sql = \"\"\"\n",
    "IMPORT INTO {schema!i}.{table!i}\n",
    "FROM SCRIPT {schema!i}.IMPORT_PATH WITH\n",
    "    BUCKET_PATH = 's3a://aws-roda-ml-datalake/yt8m_ods/vocabulary/*'\n",
    "    DATA_FORMAT = 'PARQUET'\n",
    "    S3_ENDPOINT = 's3-us-west-2.amazonaws.com'\n",
    "    CONNECTION_NAME = 'S3_CONNECTION';\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(sb_config) as conn:\n",
    "    conn.execute(sql, query_params=params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-03T13:46:41.978506Z",
     "start_time": "2023-11-03T13:46:27.899568Z"
    }
   },
   "id": "58c142629eb67bae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check that data was imported"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2106b40abdf36482"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open_pyexasol_connection(sb_config) as conn:\n",
    "    data_rows = conn.execute(\"select count(*) from {schema!i}.{table!i}\", query_params=params)\n",
    "    count = next(data_rows)[0] \n",
    "    print(f\"Loaded {count} rows\")\n",
    "    data = conn.execute(\"select * from {schema!i}.{table!i} limit 1\", query_params=params)\n",
    "    for row in data:\n",
    "        print(row)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a65f19ecc0ce237a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d2316eb463532aa1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
