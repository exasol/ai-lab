{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea9d8e0ecc5bbd3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Importing data from cloud storage\n",
    "\n",
    "In this notebook we'll use [cloud-storage-extension](https://github.com/exasol/cloud-storage-extension/) to import publicly available data from AWS S3 into the Exasol database. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912d75f3f8f8b87",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before importing the data we need to configure the database, by setting up cloud-storage-extension jar files and UDF scripts used for the import. This needs to be done once for the database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40948e6b5e139445",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Open Secure Configuration Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e09393f1b792f2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d575d602a1d347e7a30f07fef7607002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa35bc3962144befa6b3adfab142c29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Box(children=(Label(value='Configuration Store', layout=Layout(border_bottom='solid 1px', borderâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../utils/access_store_ui.ipynb\n",
    "display(get_access_store_ui('../'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f58637e1c5c7a8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Setup of cloud-storage-extension\n",
    "\n",
    "Before running the setup, you need to perform the \"Main configuration\" in [main_config.ipynb](../main_config.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d09c1457b4ae0b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could Storage Extension was initialized\n"
     ]
    }
   ],
   "source": [
    "%run ../cloud_store_config.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a313d4d1c26d3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Importing data from parquet files\n",
    "\n",
    "For the beginning, we'll load small volume of data from publicly available [Ookla Network Performance Maps](https://registry.opendata.aws/speedtest-global-performance/), which contains aggregated network performance measurements from speedtest.net website.\n",
    "\n",
    "In this example, we'll import only the subset of dataset - only mobile users for Q1 of 2019. In total there are 3M rows stored in parquet file on public S3 bucket: s3://ookla-open-data/parquet/performance/type=mobile/year=2019/quarter=1/2019-01-01_performance_mobile_tiles.parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6003fd367d36d89a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pyexasol\n",
    "from exasol.nb_connector.connections import open_pyexasol_connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc968652856f86",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Schema and target table\n",
    "\n",
    "As a first step, we need to obtain the schema of the data (set of columns stored in parquet files with their types). You might have this information in advance (if this is your dataset), but if not, you need to analyze parquet files to figure out their schema.\n",
    "\n",
    "One of the options of doing this is the parquet-tools library wrapped into a [docker container](https://hub.docker.com/r/nathanhowell/parquet-tools). To use it, you need to download one of parquet files locally, then run this docker container against this file. Using the same container, you can also peek into parquet files and looks at its actual data.\n",
    "\n",
    "For the file above, I got the following schema information:\n",
    "\n",
    "```\n",
    "message schema {\n",
    "  optional binary quadkey (STRING);\n",
    "  optional binary tile (STRING);\n",
    "  optional int64 avg_d_kbps;\n",
    "  optional int64 avg_u_kbps;\n",
    "  optional int64 avg_lat_ms;\n",
    "  optional int64 tests;\n",
    "  optional int64 devices;\n",
    "}\n",
    "```  \n",
    "\n",
    "From this schema we see that we have two types of columns in the parquet file - strings and integers.\n",
    "Let's create the table in our database for this data. The names of columns are not important, just the order and their types have to match with parquet file schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21cfe3edd3246a51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T13:46:24.373333Z",
     "start_time": "2023-11-03T13:46:24.288010Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "TABLE_NAME = \"OOKLA_MAP\"\n",
    "\n",
    "sql = \"\"\"\n",
    "create or replace table {schema_name!i}.{table_name!i} \n",
    "(\n",
    "    quadkey     VARCHAR2(1024),\n",
    "    tile        VARCHAR2(1024),\n",
    "    avg_d_kbps  BIGINT,\n",
    "    avg_u_kbps  BIGINT,\n",
    "    avg_lat_ms  BIGINT,\n",
    "    tests       BIGINT,\n",
    "    devices     BIGINT\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    conn.execute(sql, query_params={\n",
    "        \"schema_name\": ai_lab_config.db_schema,\n",
    "        \"table_name\": TABLE_NAME\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df21776e2435eced",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## S3 credentials\n",
    "\n",
    "If S3 bucket is public, we can pass empty access key and secret keys. Otherwise replace with valid credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ec486162082706",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T13:43:05.595870Z",
     "start_time": "2023-11-03T13:43:05.551031Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "CREATE OR REPLACE CONNECTION S3_CONNECTION TO '' USER '' \n",
    "IDENTIFIED BY 'S3_ACCESS_KEY={access_key!r};S3_SECRET_KEY={secret_key!r}';\n",
    "\"\"\"\n",
    "\n",
    "S3_ACCESS_KEY = \"\"\n",
    "S3_SECRET_KEY = \"\"\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    conn.execute(sql, query_params={\n",
    "        \"schema\": ai_lab_config.db_schema,\n",
    "        \"access_key\": S3_ACCESS_KEY,\n",
    "        \"secret_key\": S3_SECRET_KEY,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a05075716522ec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Importing data\n",
    "\n",
    "Now it's time to import our data. We call the `IMPORT_PATH` script, providing the location of parquet files, their format, the s3 endpoint (which has to match the bucket's configuration) and the name of our connection object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58c142629eb67bae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T13:46:41.978506Z",
     "start_time": "2023-11-03T13:46:27.899568Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"schema\": ai_lab_config.db_schema,\n",
    "    \"table\": TABLE_NAME,  \n",
    "}\n",
    "\n",
    "sql = \"\"\"\n",
    "IMPORT INTO {schema!i}.{table!i}\n",
    "FROM SCRIPT {schema!i}.IMPORT_PATH WITH\n",
    "    BUCKET_PATH = 's3a://ookla-open-data/parquet/performance/type=mobile/year=2019/quarter=1/*'\n",
    "    DATA_FORMAT = 'PARQUET'\n",
    "    S3_ENDPOINT = 's3-us-west-2.amazonaws.com'\n",
    "    CONNECTION_NAME = 'S3_CONNECTION';\n",
    "\"\"\"\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    conn.execute(sql, query_params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106b40abdf36482",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's check that data was imported by the process above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a65f19ecc0ce237a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3231245 rows\n",
      "('0212113210312302', 'POLYGON((-114.32373046875 53.1829958600872, -114.318237304688 53.1829958600872, -114.318237304688 53.1797038936054, -114.32373046875 53.1797038936054, -114.32373046875 53.1829958600872))', '39647', '29770', '30', '1', '1')\n"
     ]
    }
   ],
   "source": [
    "with open_pyexasol_connection(ai_lab_config) as conn:\n",
    "    data_rows = conn.execute(\"select count(*) from {schema!i}.{table!i}\", query_params=params)\n",
    "    count = next(data_rows)[0] \n",
    "    print(f\"Loaded {count} rows\")\n",
    "    data = conn.execute(\"select * from {schema!i}.{table!i} limit 1\", query_params=params)\n",
    "    for row in data:\n",
    "        print(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2316eb463532aa1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
