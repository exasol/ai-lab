{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab6f747-2286-4d25-834d-195c742e5b62",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/exasol/ai-lab/refs/heads/main/assets/Exasol_Logo_2025_Dark.svg\" style=\"width:200px; margin: 10px;\" />\n",
    "</div>\n",
    "\n",
    "# Working with Exasol Using the PyExasol Connector\n",
    "\n",
    "In this notebook, we will show some basic operations on data using PyExasol. You can find more detailed information on using PyExasol in its [documentation](https://exasol.github.io/pyexasol/master/index.html).\n",
    "\n",
    "The notebook is organized as a quickstart tutorial in which we will be looking at the flexibility and ease by which PyExasol allows users to import, export, and transform data both in Python and in the Exasol database. To showcase this ability, we will be using data on US flight delays. In particular, we will explore the delay caused by the carrier. We will rank the carriers using the delay as the performance metric. The data is publicly accessible at the [Bureau of Transportation Statistics](https://www.transtats.bts.gov/Homepage.asp) of the US Department of Transportation.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Prior to using this notebook the following steps need to be completed:\n",
    "1. [Configure the AI Lab](../main_config.ipynb).\n",
    "\n",
    "Please note:\n",
    "* AI Lab currently is shipped with PyExasol version `1.3.0`. The `import_from_parquet` and `export_to_parquet` were introduced in PyExasol version `1.2.0`, and the usage with polars was first introduced in PyExasol version `1.0.0`. Both functions for parquet and polars should work for all supported database versions, as these functions convert data into streamed CSVs before executing the IMPORT or EXPORT statement.\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "### 1.1 Open Secure Configuration Storage (SCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b99ef-31c4-4ea4-a362-207de124c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/access_store_ui.ipynb\n",
    "display(get_access_store_ui('../'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea053fb-3170-47a1-9d50-583be681f9cb",
   "metadata": {},
   "source": [
    "### 1.2 Download Files to Local Filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d4b50-4ecf-4475-8fd4-608f2e5122d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                \n",
    "        print(\"File downloaded successfully!\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c3970-5109-4641-83ff-0e256027725c",
   "metadata": {},
   "source": [
    "#### 1.2.1 Download Parquet File from S3 Bucket & Inspect with Pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78991ca-706f-44e3-af92-c78cdb70d50b",
   "metadata": {},
   "source": [
    "We download a prepared flights file from an AWS S3 Bucket to our local filesystem and then we load and inspect the data with pyarrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688e98c-c1fa-46bd-b7b7-05e73d7a1301",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_parquet_file = \"US_FLIGHTS_FEB_2024.parquet\"\n",
    "\n",
    "download_file(\n",
    "    url=\"https://ai-lab-example-data-s3.s3.eu-central-1.amazonaws.com/first_steps/US_FLIGHTS_FEB_2024.parquet\",\n",
    "    filename=flights_parquet_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eceb4bd-ff93-4147-aeac-e9b91a223e42",
   "metadata": {},
   "source": [
    "Let's take a look at the contents on this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbb954-01b2-4e10-b73a-6cf1b31c292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import dataset\n",
    "\n",
    "parquet_table = dataset.dataset(flights_parquet_file).to_table()\n",
    "\n",
    "parquet_table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b853a824-657d-4aca-9452-e2e1ff2b008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_table.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa5ff7-caf8-4ec8-ba50-3127b5618dfa",
   "metadata": {},
   "source": [
    "#### 1.2.2 Download CVS File from Remote Filesystem & Inspect with Polars\n",
    "This section demonstrates how to download a CSV file from a remote source.\n",
    "The data is publicly accessible at the [Bureau of Transportation Statistics](https://www.transtats.bts.gov/Homepage.asp) of the US Department of Transportation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28dbbfb-57d0-4787-a940-df26da79209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_csv_file = \"US_AIRLINES.csv\"\n",
    "\n",
    "download_file(\n",
    "    url=\"https://dut5tonqye28.cloudfront.net/ai_lab/flight-info/US_AIRLINES.csv\",\n",
    "    filename=airlines_csv_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decea784-2358-417f-9e08-855f006d5688",
   "metadata": {},
   "source": [
    "Let's take a look at the contents on this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980f738c-418f-4df8-863d-7760c46cf6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be removed once https://github.com/exasol/notebook-connector/issues/306 is resolved, and there's a new notebook-connector release.\n",
    "%pip install polars==1.35.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29492435-2114-48de-b77c-89c7313922af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "polars_dataframe = pl.read_csv(airlines_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d002880-86b4-411a-919d-808f272096a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "polars_dataframe.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d6bc9-8fab-48a1-abb1-a2704be47a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "polars_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77474ee8-e7c7-4ff8-95c9-cc79527dda0f",
   "metadata": {},
   "source": [
    "### 1.3 Create Tables\n",
    "\n",
    "We will start by creating empty tables for storing the flight delay data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e08f2-a9fb-41e0-8c38-8e0f6fa09bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class TableInfo(NamedTuple):\n",
    "    table: str\n",
    "    schema: str = ai_lab_config.db_schema\n",
    "\n",
    "    @property\n",
    "    def as_tuple(self):\n",
    "        \"\"\"This format is needed for PyExasol connections as keyword arguments\"\"\"\n",
    "        return (self.schema, self.table)\n",
    "\n",
    "    @property\n",
    "    def as_string(self):\n",
    "        \"\"\"The format format is important for completed queries sent to PyExasol\"\"\"\n",
    "        return f\"{self.schema}.{self.table}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a1a43-215b-4b2d-9e8f-e36613c83c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_airlines = TableInfo(table=\"US_AIRLINES\")\n",
    "us_flights = TableInfo(table=\"US_FLIGHTS\")\n",
    "\n",
    "\n",
    "us_airlines_ddl = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {us_airlines.as_string} (\n",
    "  OP_CARRIER_AIRLINE_ID DECIMAL(10, 0) IDENTITY PRIMARY KEY,\n",
    "  CARRIER_NAME VARCHAR(1000)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "us_flights_ddl = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {us_flights.as_string} (\n",
    "  FL_DATE TIMESTAMP, \n",
    "  OP_CARRIER_AIRLINE_ID DECIMAL(10, 0),\n",
    "  ORIGIN_AIRPORT_SEQ_ID DECIMAL(10, 0),\n",
    "  ORIGIN_STATE_ABR CHAR(2),\n",
    "  DEST_AIRPORT_SEQ_ID DECIMAL(10, 0),\n",
    "  DEST_STATE_ABR CHAR(2),\n",
    "  CRS_DEP_TIME CHAR(4),\n",
    "  DEP_DELAY DOUBLE, \n",
    "  CRS_ARR_TIME CHAR(4),\n",
    "  ARR_DELAY DOUBLE,\n",
    "  CANCELLED BOOLEAN,\n",
    "  CANCELLATION_CODE CHAR(1),\n",
    "  DIVERTED BOOLEAN,\n",
    "  CRS_ELAPSED_TIME DOUBLE,\n",
    "  ACTUAL_ELAPSED_TIME DOUBLE, \n",
    "  DISTANCE DOUBLE,\n",
    "  CARRIER_DELAY DOUBLE,\n",
    "  WEATHER_DELAY DOUBLE,\n",
    "  NAS_DELAY DOUBLE,\n",
    "  SECURITY_DELAY DOUBLE,\n",
    "  LATE_AIRCRAFT_DELAY DOUBLE\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e46ed-215c-47e3-a20c-debfc3d9ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exasol.nb_connector.connections import open_pyexasol_connection\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    for ddl in (us_airlines_ddl, us_flights_ddl):\n",
    "        conn.execute(ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc22420-531d-4586-8405-09a7c43601c0",
   "metadata": {},
   "source": [
    "### 1.4 Import Airline Information from CSV with PyExasol\n",
    "\n",
    "We use PyExasol's import_from_file to import the CSV into the US_AIRLINES table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b39f33-7ea9-4965-a496-b756dd53736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    path = Path(airlines_csv_file)\n",
    "    import_params = {\n",
    "        \"column_delimiter\": '\"',\n",
    "        \"column_separator\": \",\",\n",
    "        \"row_separator\": \"CRLF\",\n",
    "        \"skip\": 1,\n",
    "    }\n",
    "    conn.import_from_file(path, us_airlines.as_tuple, import_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e96ce3-8746-4585-9a49-96bcac6919de",
   "metadata": {},
   "source": [
    "## 2. Roundtrip with Parquet\n",
    "\n",
    "### 2.1 Importing Parquet Data from a Local Filesystem\n",
    "\n",
    "This section demonstrates how to import a parquet file from a local source into the database using PyExasol.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac58016d-2c3e-40a2-8a0d-24a9c7bf570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    conn.import_from_parquet(source=Path(flights_parquet_file), table=us_flights.as_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f74bad-9b05-4eae-a4a6-098fae91bd51",
   "metadata": {},
   "source": [
    "### 2.2 Aggregating & Transforming the Data in the Database\n",
    "\n",
    "Let's find out which airline has the highest delay per flight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1ffc6-c152-4f1a-b87c-3e2775261100",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_delay_per_flight_query = f\"\"\"\n",
    "SELECT\n",
    "  CARRIER_NAME \"Airline\",\n",
    "  SUM(CARRIER_DELAY) \"Combined Delay\",\n",
    "  COUNT(CARRIER_DELAY) \"Delayed Flights\",\n",
    "  COUNT(F.OP_CARRIER_AIRLINE_ID) \"Total flights\",\n",
    "  ROUND( SUM(CARRIER_DELAY) / COUNT(F.OP_CARRIER_AIRLINE_ID), 1 ) \"Delay per flight\"\n",
    "FROM {us_flights.as_string} F\n",
    "  JOIN {us_airlines.as_string} A \n",
    "  ON A.OP_CARRIER_AIRLINE_ID = F.OP_CARRIER_AIRLINE_ID\n",
    "WHERE NOT (CANCELLED OR DIVERTED)\n",
    "GROUP BY CARRIER_NAME\n",
    "ORDER BY \"Delay per flight\" DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae553aa-ec88-4188-a0ac-38eeb10ef285",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    result = conn.execute(highest_delay_per_flight_query).fetchall()\n",
    "\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3a94c50-3cdb-4c74-8df5-521f5c7887e3",
   "metadata": {},
   "source": [
    "### 2.3 Exporting the Transformed Data to Parquet\n",
    "\n",
    "This section demonstrates how to export transformed data to a parquet file in the local filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a089ad52-df69-433a-9cb4-a45161b02813",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_delay_parquet_directory = \"highest_delay\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f326c-0839-449a-9d85-180e9d18e2c1",
   "metadata": {},
   "source": [
    "#### 2.3.1 Save One File without Additional callback_params \n",
    "\n",
    "This code will save all of the data for the query into 1 parquet file, \n",
    "but as `callback_params={\"existing_data_behavior\":...}` was not changed, if you re-run this cell without deleting the directory, \n",
    "**you will get an exception**:\n",
    "\n",
    "```python\n",
    "ValueError: 'highest_delay' contains existing files and `callback_params['existing_data_behavior']` is not one of these values: ('overwrite_or_ignore', 'delete_matching').\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2fc785-29d4-4e44-bfb8-512432d453c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    conn.export_to_parquet(dst=highest_delay_parquet_directory, query_or_table=highest_delay_per_flight_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a36371-e369-4ec6-abab-3990eb8db479",
   "metadata": {},
   "source": [
    "#### 2.3.2 Repeatedly Save One File with callback_params[\"existing_data_behavior\"]=\"overwrite_or_ignore\"\n",
    "\n",
    "This code will save all of the data for the query into 1 parquet file and overwrite any existing & matching parquet filename.<br>\n",
    "This is due to: `callback_params[\"existing_data_behavior\"] = \"overwrite_or_ignore\"`.<br>\n",
    "**This allows the cell to be executed multiple times, unlike in 2.3.1.**\n",
    "\n",
    "\n",
    "\"existing_data_behavior\" can be set to:\n",
    "> * `error` (default value) raises an error if **any** data exists in the destination.\n",
    "\n",
    "> * `overwrite_or_ignore` will ignore any existing data and will overwrite files with the same name as an output file. Other existing files will be ignored. This behavior, in combination with a unique basename_template for each write, will allow for an append workflow                                                                                                                                                             \n",
    "                                                                                                                                                                > * `delete_matching` is useful when you are writing a partitioned dataset. The first time each partition directory is encountered the entire directory will be deleted. This allows you to overwrite old partitions completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3cd997-e8cb-496f-b234-7dbdd80ed567",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    conn.export_to_parquet(dst=highest_delay_parquet_directory, query_or_table=highest_delay_per_flight_query,\n",
    "                          callback_params={\"existing_data_behavior\": \"overwrite_or_ignore\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd019e-b58c-4646-ba8e-5b79edbf1f23",
   "metadata": {},
   "source": [
    "#### 2.3.3 Save Multiple Files with callback_params[\"max_rows_per_file\"]=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c33be-2204-4bc8-8b9f-2873529d976e",
   "metadata": {},
   "source": [
    "This code will save all of the data for the query into 3 parquet files and overwrite any existing & matching parquet filename.<br>\n",
    "The saving into multiple files comes from: `callback[\"max_rows_per_file\"]=5` and `callback[\"max_rows_per_group\"]=5` <br>\n",
    "This overwriting behavior is due to: `callback_params[\"existing_data_behavior\"] = \"overwrite_or_ignore\"`. \n",
    "\n",
    "**Note:** If ``max_rows_per_file`` is altered, ensure that ``max_rows_per_group`` is set to a value less than or equal to the value of ``max_rows_per_file``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d213f9f-f99d-439c-9a44-6cc7544a7f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    conn.export_to_parquet(dst=highest_delay_parquet_directory, query_or_table=highest_delay_per_flight_query,\n",
    "                          callback_params={\"existing_data_behavior\": \"overwrite_or_ignore\",\n",
    "                                           \"max_rows_per_file\":5,\n",
    "                                           \"max_rows_per_group\":5\n",
    "                                          })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
