{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ad35fb-b662-4cb5-8db1-11d808bd6993",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/exasol/ai-lab/refs/heads/main/assets/Exasol_Logo_2025_Dark.svg\" style=\"width:200px; margin: 10px;\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3f0d7-9149-4eae-a0d7-dc49a8e4fdc2",
   "metadata": {},
   "source": [
    "# Working with Exasol Using the PyExasol Connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64393876-08f4-446a-a73b-d28137adb2d4",
   "metadata": {},
   "source": [
    "This notebook shows basic operations on example data using PyExasol. For more details, see also the [PyExasol documentation](https://exasol.github.io/pyexasol/master/index.html).\n",
    "\n",
    "Organized as a quickstart tutorial, the notebook looks the flexibility and ease by which PyExasol allows importing data into and exporting data from an Exasol database. The notebbook also shows how to transform data both in Python and within the database. The tutorial uses Polars, PyArrow and example data on US flight delays:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf272b9-c39a-4f2a-97a1-c153e3164af2",
   "metadata": {},
   "source": [
    "* [Polars](https://pola.rs/) is a lightning-fast DataFrame library for Python. A DataFrame is a two-dimensional data structure representing data as a table with rows and columns. The [import_from_polars](https://exasol.github.io/pyexasol/1.3.0/api.html#pyexasol.ExaConnection.import_from_polars) and [export_to_polars](https://exasol.github.io/pyexasol/1.3.0/api.html#pyexasol.ExaConnection.export_to_polars) were introduced in PyExasol version `1.0.0` allowing users to quickly import & export their data into & from an Exasol Database.\n",
    "* [PyArrow](https://pola.rs/) is a cross-language development platform for in-memory data. The [import_from_parquet](https://exasol.github.io/pyexasol/1.3.0/api.html#pyexasol.ExaConnection.import_from_parquet) and [export_to_parquet](https://exasol.github.io/pyexasol/1.3.0/api.html#pyexasol.ExaConnection.export_to_parquet) were introduced in PyExasol version `1.2.0` to allow users to quickly import & export their data into & from an Exasol Database to a parquet file. A parquet file is a highly compressed, high-speed storage for massive, spreadsheet-like data.\n",
    "* The example data on US flight delays is used to explore the delay caused by the carrier. We will rank the carriers using the delay as the performance metric. This data is publicly accessible at the [Bureau of Transportation Statistics](https://www.transtats.bts.gov/Homepage.asp) of the US Department of Transportation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a044ca7-1156-46d0-9fec-0531ee69534c",
   "metadata": {},
   "source": [
    "We will use both the import and export functionalities of Polars & PyArrow in a a round trip manner with the data on US flight delays to illustrate how PyExasol creates a versatile & flexible bridge between Python code and the Exasol Database. Beyond specific use cases, like a project using Polars, this can leverage dynamic ecosystems with multiple input and output formats typical for real companies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a7b962-d71c-444a-98ec-3225f2d1be06",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Prior to using this notebook the following steps need to be completed:\n",
    "1. [Configure the AI Lab](../main_config.ipynb).\n",
    "\n",
    "Please note:\n",
    "* AI Lab currently is shipped with PyExasol version `1.3.0`. \n",
    "* Functions for PyArrow's parquet and Polars's DataFrame should work for all supported database versions, as data is converted into streamed CSVs before executing SQL statements `IMPORT` or `EXPORT`.\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "### 1.1 Open Secure Configuration Storage (SCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b99ef-31c4-4ea4-a362-207de124c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/access_store_ui.ipynb\n",
    "display(get_access_store_ui('../'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea053fb-3170-47a1-9d50-583be681f9cb",
   "metadata": {},
   "source": [
    "### 1.2 Download Files to Local Filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d4b50-4ecf-4475-8fd4-608f2e5122d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(filename, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                \n",
    "        print(\"File downloaded successfully!\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c3970-5109-4641-83ff-0e256027725c",
   "metadata": {},
   "source": [
    "#### 1.2.1 Download Parquet File from S3 Bucket & Inspect with Pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78991ca-706f-44e3-af92-c78cdb70d50b",
   "metadata": {},
   "source": [
    "In this section we download a prepared file from an AWS S3 Bucket to our local filesystem. The file contains data on US flight delays. After that, we load and inspect the data with PyArrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688e98c-c1fa-46bd-b7b7-05e73d7a1301",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_parquet_file = \"US_FLIGHTS_FEB_2024.parquet\"\n",
    "\n",
    "download_file(\n",
    "    url=\"https://ai-lab-example-data-s3.s3.eu-central-1.amazonaws.com/first_steps/US_FLIGHTS_FEB_2024.parquet\",\n",
    "    filename=flights_parquet_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eceb4bd-ff93-4147-aeac-e9b91a223e42",
   "metadata": {},
   "source": [
    "Let's take a look at the contents on this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbb954-01b2-4e10-b73a-6cf1b31c292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import dataset\n",
    "\n",
    "parquet_table = dataset.dataset(flights_parquet_file).to_table()\n",
    "\n",
    "parquet_table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b853a824-657d-4aca-9452-e2e1ff2b008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_table.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa5ff7-caf8-4ec8-ba50-3127b5618dfa",
   "metadata": {},
   "source": [
    "#### 1.2.2 Download CVS File from Remote Filesystem & Inspect with Polars\n",
    "This section demonstrates how to download a CSV file from a remote source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28dbbfb-57d0-4787-a940-df26da79209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_csv_file = \"US_AIRLINES.csv\"\n",
    "\n",
    "download_file(\n",
    "    url=\"https://dut5tonqye28.cloudfront.net/ai_lab/flight-info/US_AIRLINES.csv\",\n",
    "    filename=airlines_csv_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decea784-2358-417f-9e08-855f006d5688",
   "metadata": {},
   "source": [
    "Let's take a look at the contents of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29492435-2114-48de-b77c-89c7313922af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "polars_dataframe = pl.read_csv(airlines_csv_file)\n",
    "polars_dataframe.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d6bc9-8fab-48a1-abb1-a2704be47a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "polars_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77474ee8-e7c7-4ff8-95c9-cc79527dda0f",
   "metadata": {},
   "source": [
    "### 1.3 Create Tables\n",
    "\n",
    "We will start by creating empty tables for storing the flight delay data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e08f2-a9fb-41e0-8c38-8e0f6fa09bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class TableInfo(NamedTuple):\n",
    "    table: str\n",
    "    schema: str = ai_lab_config.db_schema\n",
    "\n",
    "    @property\n",
    "    def as_tuple(self):\n",
    "        \"\"\"This format is needed for PyExasol connections as keyword arguments\"\"\"\n",
    "        return (self.schema, self.table)\n",
    "\n",
    "    @property\n",
    "    def as_string(self):\n",
    "        \"\"\"The format format is important for completed queries sent to PyExasol\"\"\"\n",
    "        return f\"{self.schema}.{self.table}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a1a43-215b-4b2d-9e8f-e36613c83c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_airlines = TableInfo(table=\"US_AIRLINES\")\n",
    "us_flights = TableInfo(table=\"US_FLIGHTS\")\n",
    "\n",
    "\n",
    "us_airlines_ddl = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {us_airlines.as_string} (\n",
    "  OP_CARRIER_AIRLINE_ID DECIMAL(10, 0) IDENTITY PRIMARY KEY,\n",
    "  CARRIER_NAME VARCHAR(1000)\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "us_flights_ddl = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {us_flights.as_string} (\n",
    "  FLIGHT_ID DECIMAL(18,0) IDENTITY PRIMARY KEY,\n",
    "  FL_DATE TIMESTAMP, \n",
    "  OP_CARRIER_AIRLINE_ID DECIMAL(10, 0),\n",
    "  ORIGIN_AIRPORT_SEQ_ID DECIMAL(10, 0),\n",
    "  ORIGIN_STATE_ABR CHAR(2),\n",
    "  DEST_AIRPORT_SEQ_ID DECIMAL(10, 0),\n",
    "  DEST_STATE_ABR CHAR(2),\n",
    "  CRS_DEP_TIME CHAR(4),\n",
    "  DEP_DELAY DOUBLE, \n",
    "  CRS_ARR_TIME CHAR(4),\n",
    "  ARR_DELAY DOUBLE,\n",
    "  CANCELLED BOOLEAN,\n",
    "  CANCELLATION_CODE CHAR(1),\n",
    "  DIVERTED BOOLEAN,\n",
    "  CRS_ELAPSED_TIME DOUBLE,\n",
    "  ACTUAL_ELAPSED_TIME DOUBLE, \n",
    "  DISTANCE DOUBLE,\n",
    "  CARRIER_DELAY DOUBLE,\n",
    "  WEATHER_DELAY DOUBLE,\n",
    "  NAS_DELAY DOUBLE,\n",
    "  SECURITY_DELAY DOUBLE,\n",
    "  LATE_AIRCRAFT_DELAY DOUBLE\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e46ed-215c-47e3-a20c-debfc3d9ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exasol.nb_connector.connections import open_pyexasol_connection\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    for ddl in (us_airlines_ddl, us_flights_ddl):\n",
    "        conn.execute(ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc22420-531d-4586-8405-09a7c43601c0",
   "metadata": {},
   "source": [
    "### 1.4 Import Airline Information from CSV with PyExasol\n",
    "\n",
    "We use PyExasol's import_from_file to import the CSV into the US_AIRLINES table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b39f33-7ea9-4965-a496-b756dd53736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    path = Path(airlines_csv_file)\n",
    "    import_params = {\n",
    "        \"column_delimiter\": '\"',\n",
    "        \"column_separator\": \",\",\n",
    "        \"row_separator\": \"CRLF\",\n",
    "        \"skip\": 1,\n",
    "    }\n",
    "    conn.import_from_file(path, us_airlines.as_tuple, import_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e96ce3-8746-4585-9a49-96bcac6919de",
   "metadata": {},
   "source": [
    "## 2. Roundtrip with Parquet\n",
    "\n",
    "### 2.1 Importing Parquet Data from a Local Filesystem\n",
    "\n",
    "This section demonstrates how to import a parquet file from a local source into the database using PyExasol.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac58016d-2c3e-40a2-8a0d-24a9c7bf570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "columns = parquet_table.column_names\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    conn.import_from_parquet(source=Path(flights_parquet_file), table=us_flights.as_tuple, import_params={\"columns\":columns})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f74bad-9b05-4eae-a4a6-098fae91bd51",
   "metadata": {},
   "source": [
    "### 2.2 Aggregating & Transforming the Data in the Database\n",
    "\n",
    "Let's find out which airline has the highest delay per flight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1ffc6-c152-4f1a-b87c-3e2775261100",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_delay_per_flight_query = f\"\"\"\n",
    "SELECT\n",
    "  CARRIER_NAME \"Airline\",\n",
    "  SUM(CARRIER_DELAY) \"Combined Delay\",\n",
    "  COUNT(CARRIER_DELAY) \"Delayed Flights\",\n",
    "  COUNT(F.OP_CARRIER_AIRLINE_ID) \"Total flights\",\n",
    "  ROUND( SUM(CARRIER_DELAY) / COUNT(F.OP_CARRIER_AIRLINE_ID), 1 ) \"Delay per flight\"\n",
    "FROM {us_flights.as_string} F\n",
    "  JOIN {us_airlines.as_string} A \n",
    "  ON A.OP_CARRIER_AIRLINE_ID = F.OP_CARRIER_AIRLINE_ID\n",
    "WHERE NOT (CANCELLED OR DIVERTED)\n",
    "GROUP BY CARRIER_NAME\n",
    "ORDER BY \"Delay per flight\" DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae553aa-ec88-4188-a0ac-38eeb10ef285",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    result = conn.execute(highest_delay_per_flight_query).fetchall()\n",
    "\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3a94c50-3cdb-4c74-8df5-521f5c7887e3",
   "metadata": {},
   "source": [
    "### 2.3 Exporting the Transformed Data to Parquet\n",
    "\n",
    "This section demonstrates how to export transformed data to a parquet file in the local filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a089ad52-df69-433a-9cb4-a45161b02813",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_directory = \"highest_delay\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f326c-0839-449a-9d85-180e9d18e2c1",
   "metadata": {},
   "source": [
    "#### 2.3.1 Save One File without Additional callback_params \n",
    "\n",
    "This code will save all of the data for the query into 1 parquet file, \n",
    "but as `callback_params={\"existing_data_behavior\":...}` was not changed, if you re-run this cell without deleting the directory, \n",
    "**you will get an exception**:\n",
    "\n",
    "```python\n",
    "ValueError: 'highest_delay' contains existing files and `callback_params['existing_data_behavior']` is not one of these values: ('overwrite_or_ignore', 'delete_matching').\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2fc785-29d4-4e44-bfb8-512432d453c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    conn.export_to_parquet(\n",
    "        dst=local_directory, \n",
    "        query_or_table=highest_delay_per_flight_query\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a36371-e369-4ec6-abab-3990eb8db479",
   "metadata": {},
   "source": [
    "#### 2.3.2 Repeatedly Save One File with callback_params[\"existing_data_behavior\"]=\"overwrite_or_ignore\"\n",
    "\n",
    "This code will save all of the data for the query into 1 parquet file and overwrite any existing & matching parquet filename.<br>\n",
    "This is due to: `callback_params[\"existing_data_behavior\"] = \"overwrite_or_ignore\"`.<br>\n",
    "**This allows the cell to be executed multiple times, unlike in 2.3.1.**\n",
    "\n",
    "\n",
    "`existing_data_behavior` can be set to:\n",
    "> * `error` (default value) raises an error if **any** data exists in the destination.\n",
    "\n",
    "> * `overwrite_or_ignore` will ignore any existing data and will overwrite files with the same name as an output file. Other existing files will be ignored. This behavior, in combination with a unique basename_template for each write, will allow for an append workflow                                                                                                                                                             \n",
    "                                                                                                                                                                > * `delete_matching` is useful when you are writing a partitioned dataset. The first time each partition directory is encountered the entire directory will be deleted. This allows you to overwrite old partitions completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3cd997-e8cb-496f-b234-7dbdd80ed567",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    conn.export_to_parquet(\n",
    "        dst=local_directory, \n",
    "        query_or_table=highest_delay_per_flight_query,\n",
    "        callback_params={\"existing_data_behavior\": \"overwrite_or_ignore\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd019e-b58c-4646-ba8e-5b79edbf1f23",
   "metadata": {},
   "source": [
    "#### 2.3.3 Save Multiple Files with callback_params[\"max_rows_per_file\"]=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c33be-2204-4bc8-8b9f-2873529d976e",
   "metadata": {},
   "source": [
    "This code will save all of the data for the query into 3 parquet files and overwrite any existing & matching parquet filename.<br>\n",
    "The saving into multiple files comes from: `callback[\"max_rows_per_file\"]=5` and `callback[\"max_rows_per_group\"]=5` <br>\n",
    "This overwriting behavior is due to: `callback_params[\"existing_data_behavior\"] = \"overwrite_or_ignore\"`. \n",
    "\n",
    "**Note:** If ``max_rows_per_file`` is altered, ensure that ``max_rows_per_group`` is set to a value less than or equal to the value of ``max_rows_per_file``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d213f9f-f99d-439c-9a44-6cc7544a7f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    conn.export_to_parquet(\n",
    "        dst=local_directory,\n",
    "        query_or_table=highest_delay_per_flight_query,\n",
    "        callback_params={\n",
    "            \"existing_data_behavior\": \"overwrite_or_ignore\",\n",
    "            \"max_rows_per_file\":5,\n",
    "            \"max_rows_per_group\":5\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8d68c-08fd-47bb-938a-5a4ebe4b78c5",
   "metadata": {},
   "source": [
    "## 3. Roundtrip with Polars\n",
    "\n",
    "### 3.1 Exporting the Data to a Polars DataFrame\n",
    "\n",
    "This section demonstrates how to export data from an Exasol Database to a Polars DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560424a8-2a33-4726-a524-862922bc75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    highest_delay_per_flight_dataframe = conn.export_to_polars(\n",
    "        query_or_table=highest_delay_per_flight_query,\n",
    "    )\n",
    "\n",
    "highest_delay_per_flight_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164135ec-6743-48e9-adcc-d744d3a1aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_query = f\"\"\"\n",
    "SELECT\n",
    "  FLIGHT_ID,\n",
    "  TO_CHAR(FL_DATE, 'D') AS day_of_week,\n",
    "  CASE \n",
    "        WHEN TO_CHAR(FL_DATE, 'D') IN ('6', '7') THEN TRUE \n",
    "        ELSE FALSE \n",
    "    END AS is_weekend,\n",
    "  OP_CARRIER_AIRLINE_ID,\n",
    "  CAST(TO_CHAR(TO_TIMESTAMP(CRS_DEP_TIME, 'HH24MI'), 'HH24') AS INTEGER) AS CRS_DEP_HOUR,\n",
    "  ORIGIN_STATE_ABR,\n",
    "  DEST_STATE_ABR,\n",
    "  DISTANCE,\n",
    "  CASE\n",
    "      WHEN ARR_DELAY > 0 THEN 1\n",
    "      ELSE 0\n",
    "  END AS was_delayed\n",
    "FROM AI_LAB.US_FLIGHTS\n",
    "  WHERE CANCELLED IS FALSE\n",
    "  AND DIVERTED IS FALSE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc2a6bf-8eac-4abe-86e8-ccc90833f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    model_dataframe = conn.export_to_polars(query_or_table=model_query)\n",
    "\n",
    "model_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef18cb-08b6-4e49-8d7e-f579a49e4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dataframe.select(pl.col(\"WAS_DELAYED\").value_counts(sort=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49abf35b-c195-43c4-95ff-bcbb1cb564ba",
   "metadata": {},
   "source": [
    "### 3.2 Training a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a34c03-0fc1-422f-b3d0-61382dd7e6d7",
   "metadata": {},
   "source": [
    "This is a toy example of a machine learning model. It is not intended to be optimized or well-performing; many steps which you would expect in the preparation of a machine learning model were not done.\n",
    "\n",
    "First, we one-hot encode categorical values and scale large numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65adab50-bf15-4ed1-b3e0-d63b7ef2ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import set_config\n",
    "\n",
    "# Globally configure sklearn to return Polars DataFrames\n",
    "set_config(transform_output=\"polars\")\n",
    "\n",
    "# Apply different transformations to different columns\n",
    "cat_cols = ['OP_CARRIER_AIRLINE_ID', 'ORIGIN_STATE_ABR', 'DEST_STATE_ABR']\n",
    "num_cols = ['DISTANCE']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(sparse_output=False), cat_cols),\n",
    "        ('num', StandardScaler(), num_cols)  # Added scaling for DISTANCE\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    # This removes the prefixes like 'remainder__' and 'cat__'\n",
    "    verbose_feature_names_out=False \n",
    ")\n",
    "\n",
    "# Transform directly using the Polars DataFrame\n",
    "model_polars = preprocessor.fit_transform(model_dataframe)\n",
    "\n",
    "model_polars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dac1df-6b98-4110-84f2-e29c520b8fea",
   "metadata": {},
   "source": [
    "Next, we split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc46db-1d61-4283-a6e9-2f0be5048f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = model_polars.drop('WAS_DELAYED', 'FLIGHT_ID')\n",
    "y = model_polars.select('WAS_DELAYED')\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf680ae-d40e-4882-a0b3-5d5f41c6aae6",
   "metadata": {},
   "source": [
    "Then, we train a random forest on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27293d-d387-47c1-a413-8ed52ebe3f05",
   "metadata": {},
   "source": [
    "Finally, we evaluate how our model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d8f689-dc46-47ff-9941-34ca7e777959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "clf_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "\n",
    "clf_model.fit(X_train, y_train.to_series())\n",
    "\n",
    "pred_valid_clf = clf_model.predict(X_valid)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_valid, pred_valid_clf):.2%}\")\n",
    "print(classification_report(y_valid, pred_valid_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b13bf8-83d6-4517-b171-945ff2579a91",
   "metadata": {},
   "source": [
    "### 3.3 Importing the Model Results into an Exasol Database Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49049082-8bf6-4b52-9fdd-3326f0337998",
   "metadata": {},
   "source": [
    "We run the machine learning model on all of our flight data and insert it into an Exasol Database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf69c0-a46a-43f7-aad2-18075dc0db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = clf_model.predict(model_polars.drop('WAS_DELAYED', 'FLIGHT_ID'))\n",
    "\n",
    "final_df = model_polars.with_columns(\n",
    "    pl.Series(\"WAS_DELAYED_PRED\", all_preds)\n",
    ").select([\n",
    "    \"FLIGHT_ID\",\n",
    "    \"WAS_DELAYED\",\n",
    "    \"WAS_DELAYED_PRED\"\n",
    "])\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e32d81-f3bf-4120-adae-72bf4a889d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_flights_model = TableInfo(table=\"US_FLIGHTS_MODEL\")\n",
    "\n",
    "us_flights_model_ddl = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {us_flights_model.as_string}(\n",
    "    FLIGHT_ID DECIMAL(18,0) NOT NULL,\n",
    "    WAS_DELAYED BOOLEAN,\n",
    "    WAS_DELAYED_PRED BOOLEAN,\n",
    "    CONSTRAINT fk_flight_id\n",
    "        FOREIGN KEY (FLIGHT_ID) \n",
    "        REFERENCES {us_flights.as_string} (FLIGHT_ID)\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293dc16-f6d7-447c-8479-68e27eb177ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    conn.execute(us_flights_model_ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a4bdb5-b45e-4a1a-ac3d-d0dff9489fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    conn.import_from_polars(\n",
    "        src=final_df,\n",
    "        table=us_flights_model.as_tuple\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dcf459-0ca2-4864-97b0-055915265544",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    df = conn.export_to_pandas(us_flights_model.as_tuple)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
