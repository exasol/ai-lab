{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0faec19c-8e4f-4ae8-8772-38dcca47d9e4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/exasol/ai-lab/refs/heads/main/assets/Exasol_Logo_2025_Dark.svg\" style=\"width:200px; margin: 10px;\" />\n",
    "</div>\n",
    "\n",
    "# Question answering model\n",
    "\n",
    "In this notebook, we will load and use a question-answering language model that can retrieve the answer to a question from a given text. Learn more about the Question Answering task <a href=\"https://huggingface.co/tasks/question-answering\" target=\"_blank\" rel=\"noopener\">here</a>. Please also refer to the Transformer Extension <a href=\"https://github.com/exasol/transformers-extension/blob/main/doc/user_guide/user_guide.md\" target=\"_blank\" rel=\"noopener\">User Guide</a> to find more information about the UDF used in this notebook.\n",
    "\n",
    "We will be running SQL queries using <a href=\"https://jupysql.ploomber.io/en/latest/quick-start.html\" target=\"_blank\" rel=\"noopener\"> JupySQL</a> SQL Magic.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Prior to using this notebook the following steps need to be completed:\n",
    "1. [Configure the AI Lab](../main_config.ipynb).\n",
    "2. [Initialize the Transformer Extension](te_init.ipynb).\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Open Secure Configuration Storage"
   ]
  },
  {
   "cell_type": "code",
   "id": "264c9323-7f40-40ca-93cf-db4853470206",
   "metadata": {},
   "source": [
    "%run ../utils/access_store_ui.ipynb\n",
    "display(get_access_store_ui('../'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd5e24a7-ecde-42cb-bdff-7df4fc4f5c84",
   "metadata": {},
   "source": [
    "Let's bring up JupySQL and connect to the database via SQLAlchemy. Please refer to the documentation of <a href=\"https://github.com/exasol/sqlalchemy-exasol\" target=\"_blank\" rel=\"noopener\">sqlalchemy-exasol</a> for details on how to connect to the database using the Exasol SQLAlchemy driver."
   ]
  },
  {
   "cell_type": "code",
   "id": "13fa8443-17f5-4f75-8c2a-3a86d13d7911",
   "metadata": {},
   "source": [
    "%run ../utils/jupysql_init.ipynb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b60ef68c-3556-4742-91b9-836b31699e4c",
   "metadata": {},
   "source": [
    "## Get a language model\n",
    "\n",
    "To demonstrate the question-answering task we will use the [roberta model](https://huggingface.co/deepset/roberta-base-squad2).\n",
    "\n",
    "We need to load the model from the Hugging Face Hub into the [BucketFS](https://docs.exasol.com/db/latest/database_concepts/bucketfs/bucketfs.htm). This could potentially be a long process, depending on the connection of the Database. Unfortunately, we cannot tell exactly when it has finished. The notebook's hourglass may not be a reliable indicator. BucketFS will still be doing some work when the call issued by the notebook returns. Please wait for a few moments after that, before querying the model.\n",
    "\n",
    "You might see a warning that some weights are newly initialized and the model should be trained on a down-stream task. Please ignore this warning. For the purpose of this demonstration, it is not important, the model should still be able to produce some meaningful output."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from exasol.nb_connector.model_installation import install_model, TransformerModel\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# This is the name of the model at the Hugging Face Hub\n",
    "MODEL_NAME = 'deepset/roberta-base-squad2'\n",
    "install_model(ai_lab_config, TransformerModel(MODEL_NAME, 'question_answering', AutoModelForQuestionAnswering))"
   ],
   "id": "aebc949e60ff5ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Use the language model\n",
    "\n",
    "We are going to check the model output given the same question but two different contexts, using the `TE_QUESTION_ANSWERING_UDF`. In neither case the context has a direct answer to the question. We expect the answer to be relevant to the context."
   ],
   "id": "29e0d88c4e9185d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This will be our question\n",
    "TEST_QUESTION = 'What is bitumen used for?'\n",
    "\n",
    "# Let's first try it first with the following context\n",
    "TEST_CONTEXT1 = \"\"\"\n",
    "Apart from the stylish design features of new flat roofs, the other thing that’s moved on considerably is the technology\n",
    "used to keep them weather-proof. Once flat roofs were notoriously prone to leaking and the problem could only be solved\n",
    "with a boiling cauldron of tar. These days there are patch repair kits, liquid rubber membranes, and even quick,\n",
    "efficient waterproofing paint that lasts for ages – and can even be applied in damp weather.\n",
    "\"\"\"\n",
    "\n",
    "# Make sure our texts can be used in an SQL statement.\n",
    "TEST_QUESTION = TEST_QUESTION.replace(\"'\", \"''\")\n",
    "TEST_CONTEXT1 = TEST_CONTEXT1.replace(\"'\", \"''\")"
   ],
   "id": "bafb631109c697fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43f3f6e9-6f40-49b2-bbba-8954c35b5e06",
   "metadata": {},
   "source": [
    "The udf takes various input parameters:\n",
    "\n",
    "* device_id: To run on a GPU, specify the valid cuda device ID.\n",
    "* bucketfs_conn: The BucketFS connection name.\n",
    "* sub_dir: The directory where the model is stored in the BucketFS.\n",
    "* model_name: The name of the model to use for prediction.\n",
    "* question: The question text.\n",
    "* context_text: The context text, associated with the question.\n",
    "* top_k: The max number of answers to return.\n",
    "\n",
    "You need to supply these parameters in the correct order. Further information can be found in the  <a href=\"https://github.com/exasol/transformers-extension/blob/main/doc/user_guide/user_guide.md\" target=\"_blank\" rel=\"noopener\">User Guide</a>.\n",
    "\n",
    "\n",
    "We will collect at most the 5 best answers.\n",
    "We will save the result in the variable `udf_output` to support automatic testing of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "id": "346b1b8a-b1aa-4bea-b351-83b8ee6ab7b7",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "running_model"
    ]
   },
   "source": [
    "%%sql\n",
    "WITH MODEL_OUTPUT AS\n",
    "(\n",
    "    SELECT TE_QUESTION_ANSWERING_UDF(\n",
    "        NULL,\n",
    "        '{{ai_lab_config.bfs_connection_name}}',\n",
    "        '{{ai_lab_config.bfs_model_subdir}}',\n",
    "        '{{MODEL_NAME}}',\n",
    "        '{{TEST_QUESTION}}',\n",
    "        '{{TEST_CONTEXT1}}',\n",
    "        5\n",
    "    )\n",
    ")\n",
    "SELECT answer, score, error_message FROM MODEL_OUTPUT ORDER BY SCORE DESC"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "061139b0-2f3c-41c5-8176-d83789cb39e5",
   "metadata": {},
   "source": [
    "As you can see, we select only some of the udf's output columns in these examples.  If you need more details to your output, you can find information on all output columns in the <a href=\"https://github.com/exasol/transformers-extension/blob/main/doc/user_guide/user_guide.md\" target=\"_blank\" rel=\"noopener\">User Guide</a>.\n",
    "\n",
    "The output of the model is sorted into the following columns by the udf:\n",
    "\n",
    "* answer: the generated answer for the input question\n",
    "* score: the confidence of the answer\n",
    "* rank: the rank of the answer. In this context, all answers for one input are ranked by their score. rank=1 means best result/highest score.\n",
    "* error_message: error occurring while executing the udf will be saved here\n",
    "\n",
    "Let's now change the context and see a different set of answers."
   ]
  },
  {
   "cell_type": "code",
   "id": "0787b6ac-f3db-4971-b010-0eac38bd1921",
   "metadata": {},
   "source": [
    "# New context\n",
    "TEST_CONTEXT2 = \"\"\"\n",
    "You can make a wooden planter in a day, using treated timber. Simply work out how big an area you need,\n",
    "cut the wood to size and follow our steps to putting the planter together. Make sure your wooden planter\n",
    "has drainage holes, so plants don’t become waterlogged.\n",
    "\"\"\"\n",
    "\n",
    "# Make sure our text can be used in an SQL statement.\n",
    "TEST_CONTEXT2 = TEST_CONTEXT2.replace(\"'\", \"''\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "07b71013-b0ae-44c9-a299-ade8e307213c",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "running_model"
    ]
   },
   "source": [
    "%%sql\n",
    "WITH MODEL_OUTPUT AS\n",
    "(\n",
    "    SELECT TE_QUESTION_ANSWERING_UDF(\n",
    "        NULL,\n",
    "        '{{ai_lab_config.bfs_connection_name}}',\n",
    "        '{{ai_lab_config.bfs_model_subdir}}',\n",
    "        '{{MODEL_NAME}}',\n",
    "        '{{TEST_QUESTION}}',\n",
    "        '{{TEST_CONTEXT2}}',\n",
    "        5\n",
    "    )\n",
    ")\n",
    "SELECT answer, score, error_message FROM MODEL_OUTPUT ORDER BY SCORE DESC"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "64bb5eca-b4e0-447c-bafd-f273c7c53a70",
   "metadata": {},
   "source": [
    "The code above shows how the model works on a toy example. However, the main purpose of having a model deployed in the database is to get a quick response for a batch input. The performance gain comes from two factors - localization and parallelization. The first means that the input data never crosses the machine boundaries. The second means that multiple instances of the model are processing the data on all available nodes in parallel.\n",
    "\n",
    "Another advantage of making predictions within the database is enhanced data security. The task of safeguarding privacy can be simplified given the fact that the source data never leaves the database machine.\n",
    "\n",
    "In a more practical application, the question and the context would be stored in columns of a database table. For example, if we wanted to get the best answer for each row of the input table `MY_TEXT_TABLE`, where the question is in the column `MY_QUESTION` and the context is in the column `MY_CONTEXT`, the SQL would look similar to this:\n",
    "```\n",
    "SELECT TE_QUESTION_ANSWERING_UDF(..., MY_QUESTION, MY_CONTEXT, 1) FROM MY_TEXT_TABLE;\n",
    "```\n",
    "Please note, that the response time observed on the provided example with a single input will not be scaled up linearly in case of multiple inputs. Much of the latency falls on loading the model into the CPU memory from BucketFS. This needs to be done only once regardless of the number of inputs."
   ]
  },
  {
   "cell_type": "code",
   "id": "e3486cfa-53e2-47be-a445-cd6c13e654bb",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
