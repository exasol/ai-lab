{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91051dad-8d45-435f-b2c9-7572e4050f58",
   "metadata": {},
   "source": [
    "# First Steps\n",
    "\n",
    "This notebook guides you through the first steps using Jupyter notebooks with Exasol.\n",
    "\n",
    "The notebook demonstrates connecting to an Exasol database instance and using some of its features. \n",
    "\n",
    "## 1. Open Secure Configuration Storage\n",
    "\n",
    "First we need to open the Secure Configuration Storage (SCS) containing the connection information such as the database host, user, password, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5970a003-896d-4f06-83ee-be98129c29d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/access_store_ui.ipynb\n",
    "display(get_access_store_ui())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15075fa-8c9d-4e60-82d9-28d92d38ee0e",
   "metadata": {},
   "source": [
    "## 2. Activate JupySQL\n",
    "\n",
    "First we will activate the [JupySQL](https://jupysql.ploomber.io) magics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73122275-3f4a-4981-b8b3-ee6c49734996",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/jupysql_init.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a7800-253d-43a0-9bd7-89932876317d",
   "metadata": {},
   "source": [
    "### 2.1 Create a Database Table\n",
    "\n",
    "We will use JupySQL to create 2 database tables but other sections will use the tables, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f5ec4-10f3-49e8-a6e4-48552a760542",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TABLE US_AIRLINES (\n",
    "        OP_CARRIER_AIRLINE_ID DECIMAL(10, 0) IDENTITY PRIMARY KEY,\n",
    "        CARRIER_NAME VARCHAR(1000)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd466ee7-7b61-4b3c-8f13-a056be60c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE TABLE US_FLIGHTS (\n",
    "        FL_DATE DATE,\n",
    "        OP_CARRIER_AIRLINE_ID DECIMAL(10, 0),\n",
    "        ORIGIN_AIRPORT_SEQ_ID DECIMAL(10, 0),\n",
    "        ORIGIN_STATE_ABR CHAR(2),\n",
    "        DEST_AIRPORT_SEQ_ID DECIMAL(10, 0),\n",
    "        DEST_STATE_ABR CHAR(2),\n",
    "        CRS_DEP_TIME CHAR(4),\n",
    "        DEP_DELAY DECIMAL(6, 2),\n",
    "        CRS_ARR_TIME CHAR(4),\n",
    "        ARR_DELAY DECIMAL(6, 2),\n",
    "        CANCELLED BOOLEAN,\n",
    "        CANCELLATION_CODE CHAR(1),\n",
    "        DIVERTED BOOLEAN,\n",
    "        CRS_ELAPSED_TIME DECIMAL(6, 2),\n",
    "        ACTUAL_ELAPSED_TIME DECIMAL(6, 2),\n",
    "        DISTANCE DECIMAL(6, 2),\n",
    "        CARRIER_DELAY DECIMAL(6, 2),\n",
    "        WEATHER_DELAY DECIMAL(6, 2),\n",
    "        NAS_DELAY DECIMAL(6, 2),\n",
    "        SECURITY_DELAY DECIMAL(6, 2),\n",
    "        LATE_AIRCRAFT_DELAY DECIMAL(6, 2)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f83db93-e196-47ae-8e41-48eb21f50f41",
   "metadata": {},
   "source": [
    "### 2.2 Importing CSV Files From Remote\n",
    "\n",
    "This section demonstrates how to import CSV files from a remote source into the database.\n",
    "\n",
    "First we will import a list of US airlines. The data is publicly accessible at the [Bureau of Transportation Statistics](https://www.transtats.bts.gov/Homepage.asp) of the US Department of Transportation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b0486-0871-44ab-a96e-66e875a28842",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "IMPORT INTO US_AIRLINES FROM \n",
    "  CSV AT 'https://dut5tonqye28.cloudfront.net/ai_lab/flight-info/' \n",
    "  FILE 'US_AIRLINES.csv'\n",
    "  COLUMN SEPARATOR = ','\n",
    "  ROW SEPARATOR = 'CRLF'\n",
    "  COLUMN DELIMITER = '\"'\n",
    "  SKIP = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2428c14-dd23-4891-8c2f-533f5c2b8f05",
   "metadata": {},
   "source": [
    "Next, we will import data about flights in February 2024:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b76b4f-167b-4769-b454-49fa2871a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "IMPORT INTO US_FLIGHTS \n",
    "    FROM CSV AT 'https://dut5tonqye28.cloudfront.net/ai_lab/first_steps/' \n",
    "    FILE 'US_FLIGHTS_FEB_2024-fixed-booleans.csv'\n",
    "    (1 FORMAT = 'MM/DD/YYYY HH12:MI:SS AM', 2..21)    \n",
    "    SKIP = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b764fd6-c3d9-49b8-bd0a-b36c6c382146",
   "metadata": {},
   "source": [
    "Let's find out which is the airline with the highest delay per flight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b5053-72c8-4846-941b-57d7dc267f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql --save udf_output\n",
    "SELECT\n",
    "    CARRIER_NAME \"Airline\"\n",
    "    , SUM(CARRIER_DELAY) \"Combined Delay\"\n",
    "    , COUNT(CARRIER_DELAY) \"Delayed Flights\"\n",
    "    , COUNT(F.OP_CARRIER_AIRLINE_ID) \"Total flights\"\n",
    "    , ROUND(sum(CARRIER_DELAY) / COUNT(F.OP_CARRIER_AIRLINE_ID), 1) \"Delay per flight\"\n",
    "    FROM US_FLIGHTS F\n",
    "    JOIN US_AIRLINES A ON A.OP_CARRIER_AIRLINE_ID = F.OP_CARRIER_AIRLINE_ID\n",
    "    WHERE NOT (CANCELLED OR DIVERTED)\n",
    "    GROUP BY CARRIER_NAME\n",
    "    ORDER BY \"Delay per flight\" DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ff5f1-1eb8-452c-8827-6c675c71dba5",
   "metadata": {},
   "source": [
    "### 2.3 Importing a Parquet File From an AWS S3 Bucket\n",
    "\n",
    "Exasol can import Parquet files from AWS S3 Buckets, see [docs.exasol.com](https://docs.exasol.com/db/latest/loading_data/load_data_parquet.htm).\n",
    "This demo uses a file already uploaded to S3 bucket `ai-lab-example-data-s3`.\n",
    "\n",
    "First we will define a connection pointing to the S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de58f6-b561-45c3-bc96-38523fcff58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE CONNECTION AI_LAB_FIRST_STEPS_S3\n",
    "    TO 'https://ai-lab-example-data-s3.s3.eu-central-1.amazonaws.com';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23507a42-9459-4446-af7f-7d4f65294242",
   "metadata": {},
   "source": [
    "Alternatively the connection can also use the following URL syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945dc2dd-8722-4455-b77e-cf6ad88eea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE OR REPLACE CONNECTION AI_LAB_FIRST_STEPS_S3 TO 's3://ai-lab-example-data-s3';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118dd1a-40e6-412a-8624-314e7de1f893",
   "metadata": {},
   "source": [
    "Then we will remove the data imported before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba8cad-8451-4f7a-8dec-3d04950d88b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "TRUNCATE TABLE US_FLIGHTS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc80445-e5c0-43c1-ae34-f16326d6fb94",
   "metadata": {},
   "source": [
    "Now we can import the Parquet file from S3 into the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa97292-b0e5-441b-a961-a3d1fba6d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "IMPORT INTO US_FLIGHTS\n",
    "    FROM PARQUET AT AI_LAB_FIRST_STEPS_S3\n",
    "    FILE 'first_steps/US_FLIGHTS_FEB_2024.parquet';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074a883-ae21-479c-b4fa-17962afe8149",
   "metadata": {},
   "source": [
    "We will query table `US_FLIGHTS` again to display the imported data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064158a4-8d37-42ab-905f-e13d6c2af3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM US_FLIGHTS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582ac55-9c56-47d9-b286-948835d976c4",
   "metadata": {},
   "source": [
    "## 3. PyExasol\n",
    "\n",
    "### 3.1 Importing a CSV File From the Local Filesystem\n",
    "\n",
    "This section demonstrates how to import a CSV file from the local file system into the database using pyexasol.\n",
    "\n",
    "Function `open_pyexasol_connection()` opens a connection, using the configuration from the SCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40546944-dd3e-4330-a9c0-8ef57c4b5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from exasol.nb_connector.connections import open_pyexasol_connection\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    path = Path(\"first_steps/pyexasol.csv\")\n",
    "    import_params = {\n",
    "        \"column_delimiter\": '\"',\n",
    "        \"column_separator\": \",\",\n",
    "        \"row_separator\": \"CRLF\",\n",
    "        \"skip\": 1,\n",
    "    }\n",
    "    conn.import_from_file(path, (ai_lab_config.db_schema, \"US_AIRLINES\"), import_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023d10d-ccdb-4c2d-8fa5-655e227e8c76",
   "metadata": {},
   "source": [
    "Let's verify successful import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773a861-8399-4a17-9fe7-d1ac2e960f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM US_AIRLINES WHERE CARRIER_NAME LIKE '% local CSV file via pyexasol'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c96353d-4431-4cf3-99d2-0e5356721886",
   "metadata": {},
   "source": [
    "### 3.2 Importing a CSV File From Remote\n",
    "\n",
    "This section demonstrates how to import a CSV file from a remote source into the database using pyexasol.\n",
    "    \n",
    "First we define an SQL statement for the remote import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e84aae48-30f3-4e6b-ba1a-29644094d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_remote_csv_sql = f\"\"\"\n",
    "    IMPORT INTO \"{ai_lab_config.db_schema}\".\"US_FLIGHTS\" FROM \n",
    "    CSV AT 'https://dut5tonqye28.cloudfront.net/ai_lab/first_steps/' \n",
    "    FILE 'US_FLIGHTS_FEB_2024-fixed-booleans.csv'\n",
    "    (1 FORMAT = 'MM/DD/YYYY HH12:MI:SS AM', 2..21) \n",
    "    SKIP = 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5f12c-5465-4580-b2ff-87e71f28238c",
   "metadata": {},
   "source": [
    "This SQL statement can be reused in other examples.\n",
    "\n",
    "Then we will truncate table `US_FLIGHTS` to be able to import the flight data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fbedb-e070-4821-a8df-00cc964075d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "TRUNCATE TABLE US_FLIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324056a-65ad-4bfa-b044-d41154547d85",
   "metadata": {},
   "source": [
    "Now let's run the import via pyexasol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04364f78-a1f2-49ec-9ec9-959a2604a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exasol.nb_connector.connections import open_pyexasol_connection\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    result = conn.execute(import_remote_csv_sql)\n",
    "    print(f\"Imported {result.rowcount()} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b8024f-69c1-4247-ab5a-946d5e98fe2b",
   "metadata": {},
   "source": [
    "## 4. SQL Alchemy\n",
    "\n",
    "### 3.1 Importing a CSV File From the Local Filesystem\n",
    "\n",
    "This section demonstrates how to import a CSV file from the local file system into the database using SQL Alchemy.\n",
    "\n",
    "Function `open_sqlalchemy_connection()` returns a SQL Alchemy engine, again using the configuration from the SCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a9139-f8e1-45bd-8ea8-43571d2ee3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exasol.nb_connector.connections import open_sqlalchemy_connection\n",
    "engine = open_sqlalchemy_connection(ai_lab_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7e497-cf0a-4158-b656-221a80086a90",
   "metadata": {},
   "source": [
    "### 3.2 Importing a CSV File From Remote\n",
    "\n",
    "This section demonstrates how to import a CSV file from a remote source into the database using SQL Alchemy.\n",
    "\n",
    "First we will truncate table `US_FLIGHTS` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b831c676-b307-4af0-9b55-0ad563cfdeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "TRUNCATE TABLE US_FLIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d228759-c857-4b28-8b93-ca9baae3bc54",
   "metadata": {},
   "source": [
    "Next we will import the flight data once again, now u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce8489-e920-43ce-a9fc-3aee01ba6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusing variable import_remote_csv_sql defined in pyexasol example above\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(import_remote_csv_sql)\n",
    "print(f\"Imported {result.rowcount} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ca230-817c-4226-a1b6-ac5c32d4b1c7",
   "metadata": {},
   "source": [
    "## 5. Using the Exasol Bucket File System\n",
    "\n",
    "The [Exasol Bucket File System](https://docs.exasol.com/db/latest/database_concepts/bucketfs/bucketfs.htm) (BucketFS) is a powerful feature for exchanging non-relational data with the database nodes in an Exasol cluster.\n",
    "\n",
    "Such data can be arbitrary files including \n",
    "* Data to be processed by [User Defined Scripts](https://docs.exasol.com/db/latest/database_concepts/udf_scripts.htm) (UDFs)\n",
    "* [Script-Language Containers](https://github.com/exasol/script-languages-release) (SLCs)\n",
    "* Pretrained Large Language AI Models\n",
    "\n",
    "### 5.1 Uploading a File to the BucketFS\n",
    "\n",
    "First we will create a sample file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da84a6-c1bd-4159-830e-88a2fa94ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile first_steps/text_file.txt\n",
    "Hello World!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644c5311-27e4-463c-b228-c44ce810b50e",
   "metadata": {},
   "source": [
    "And now, let's upload the file into the BucketFS.\n",
    "\n",
    "Function `open_bucketfs_location()` returns a cursor into Exasols BucketFS, also using the configuration in the SCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ab083-d7db-4e76-8e1d-7cf7ff369c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exasol.nb_connector.connections import open_bucketfs_location\n",
    "\n",
    "file = Path(\"first_steps/text_file.txt\")\n",
    "bfs = open_bucketfs_location(ai_lab_config)\n",
    "remote = bfs / file.name\n",
    "remote.write(file.read_bytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc65f7-4d27-41b2-835a-100aa8b79d64",
   "metadata": {},
   "source": [
    "### 5.2 Listing the Files in the BucketFS \n",
    "\n",
    "We can also list all the files currently available in the BucketFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597aa88-5c37-4d2d-ae96-be90bf1a5baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfs = open_bucketfs_location(ai_lab_config)\n",
    "for p in bfs.iterdir():\n",
    "    print(f'- {p.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ff5dd-cf57-4b51-aeff-26bf1d879a85",
   "metadata": {},
   "source": [
    "### 5.3 Reading a File in the BucketFS\n",
    "\n",
    "We can also read the contents of a file in the BucketFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d3b55-04d1-4fc1-a307-383a64eed012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import exasol.bucketfs as bfs\n",
    "\n",
    "content = bfs.as_string(remote.read())\n",
    "print(f'The file in the BucketFS contains:\\n{content}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
