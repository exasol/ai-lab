{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91051dad-8d45-435f-b2c9-7572e4050f58",
   "metadata": {},
   "source": [
    "# First Steps\n",
    "\n",
    "This notebook guides you through the first steps using Jupyter notebooks with Exasol.\n",
    "\n",
    "The notebook demonstrates connecting to an Exasol database instance and using some of its features. \n",
    "\n",
    "## 1. Open Secure Configuration Storage\n",
    "\n",
    "First we need to open the Secure Configuration Storage (SCS) containing the connection information such as the database host, user, password, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5970a003-896d-4f06-83ee-be98129c29d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/access_store_ui.ipynb\n",
    "display(get_access_store_ui())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15075fa-8c9d-4e60-82d9-28d92d38ee0e",
   "metadata": {},
   "source": [
    "## 2. Using JupySQL\n",
    "\n",
    "First we will activate the [JupySQL](https://jupysql.ploomber.io) magics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73122275-3f4a-4981-b8b3-ee6c49734996",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/jupysql_init.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a7800-253d-43a0-9bd7-89932876317d",
   "metadata": {},
   "source": [
    "In the background JupySQL uses SQLAlchemy, see also the [demo section on SQLAlchemy](#4.-SQLAlchemy) below.\n",
    "\n",
    "### 2.1 Create Database Tables\n",
    "\n",
    "We will use JupySQL to create 2 database tables but other sections will use the tables, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f5ec4-10f3-49e8-a6e4-48552a760542",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE\n",
    "OR REPLACE TABLE US_AIRLINES (\n",
    "  OP_CARRIER_AIRLINE_ID DECIMAL(10, 0) IDENTITY PRIMARY KEY,\n",
    "  CARRIER_NAME VARCHAR(1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd466ee7-7b61-4b3c-8f13-a056be60c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE\n",
    "OR REPLACE TABLE US_FLIGHTS (\n",
    "  FL_DATE TIMESTAMP, -- was DATE\n",
    "  OP_CARRIER_AIRLINE_ID DECIMAL(10, 0),\n",
    "  ORIGIN_AIRPORT_SEQ_ID DECIMAL(10, 0),\n",
    "  ORIGIN_STATE_ABR CHAR(2),\n",
    "  DEST_AIRPORT_SEQ_ID DECIMAL(10, 0),\n",
    "  DEST_STATE_ABR CHAR(2),\n",
    "  CRS_DEP_TIME CHAR(4),\n",
    "  DEP_DELAY DOUBLE, -- was DECIMAL(6,2)\n",
    "  CRS_ARR_TIME CHAR(4),\n",
    "  ARR_DELAY DOUBLE, -- was DECIMAL(6,2)\n",
    "  CANCELLED BOOLEAN,\n",
    "  CANCELLATION_CODE CHAR(1),\n",
    "  DIVERTED BOOLEAN,\n",
    "  CRS_ELAPSED_TIME DOUBLE, -- was DECIMAL(6,2)\n",
    "  ACTUAL_ELAPSED_TIME DOUBLE, -- was DECIMAL(6,2)\n",
    "  DISTANCE DOUBLE, -- was DECIMAL(6,2)\n",
    "  CARRIER_DELAY DOUBLE, -- was DECIMAL(6,2)\n",
    "  WEATHER_DELAY DOUBLE, -- was DECIMAL(6,2)\n",
    "  NAS_DELAY DOUBLE, -- was DECIMAL(6,2)\n",
    "  SECURITY_DELAY DOUBLE, -- was DECIMAL(6,2)\n",
    "  LATE_AIRCRAFT_DELAY DOUBLE -- was DECIMAL(6,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f83db93-e196-47ae-8e41-48eb21f50f41",
   "metadata": {},
   "source": [
    "### 2.2 Importing CSV Files From Remote\n",
    "\n",
    "This section demonstrates how to import CSV files from a remote source into the database.\n",
    "\n",
    "First we will import a list of US airlines. The data is publicly accessible at the [Bureau of Transportation Statistics](https://www.transtats.bts.gov/Homepage.asp) of the US Department of Transportation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b0486-0871-44ab-a96e-66e875a28842",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "IMPORT INTO US_AIRLINES FROM\n",
    "    CSV AT 'https://dut5tonqye28.cloudfront.net/ai_lab/flight-info/' \n",
    "    FILE 'US_AIRLINES.csv' \n",
    "    COLUMN SEPARATOR = ',' \n",
    "    ROW SEPARATOR = 'CRLF'\n",
    "    COLUMN DELIMITER = '\"' \n",
    "    SKIP = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2428c14-dd23-4891-8c2f-533f5c2b8f05",
   "metadata": {},
   "source": [
    "Next, we will import data about flights in February 2024:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b76b4f-167b-4769-b454-49fa2871a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "IMPORT INTO US_FLIGHTS \n",
    "    FROM CSV AT 'https://dut5tonqye28.cloudfront.net/ai_lab/first_steps/' \n",
    "    FILE 'US_FLIGHTS_FEB_2024-fixed-booleans.csv'\n",
    "    (1 FORMAT = 'MM/DD/YYYY HH12:MI:SS AM', 2..21)    \n",
    "    SKIP = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b764fd6-c3d9-49b8-bd0a-b36c6c382146",
   "metadata": {},
   "source": [
    "Let's find out which is the airline with the highest delay per flight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b5053-72c8-4846-941b-57d7dc267f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql --save udf_output\n",
    "SELECT\n",
    "  CARRIER_NAME \"Airline\",\n",
    "  SUM(CARRIER_DELAY) \"Combined Delay\",\n",
    "  COUNT(CARRIER_DELAY) \"Delayed Flights\",\n",
    "  COUNT(F.OP_CARRIER_AIRLINE_ID) \"Total flights\",\n",
    "  ROUND( SUM(CARRIER_DELAY) / COUNT(F.OP_CARRIER_AIRLINE_ID), 1 ) \"Delay per flight\"\n",
    "FROM US_FLIGHTS F\n",
    "  JOIN US_AIRLINES A ON A.OP_CARRIER_AIRLINE_ID = F.OP_CARRIER_AIRLINE_ID\n",
    "WHERE NOT (CANCELLED OR DIVERTED)\n",
    "GROUP BY CARRIER_NAME\n",
    "ORDER BY \"Delay per flight\" DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ff5f1-1eb8-452c-8827-6c675c71dba5",
   "metadata": {},
   "source": [
    "### 2.3 Importing a Parquet File From an AWS S3 Bucket\n",
    "\n",
    "This demo uses a file already uploaded to S3 bucket `ai-lab-example-data-s3`.\n",
    "\n",
    "**Please note**: Parquet import requires using **<span style=\"color: #40a\">Exasol version 2025 or higher</span>**, see [docs.exasol.com](https://docs.exasol.com/db/latest/loading_data/load_data_parquet.htm).\n",
    "\n",
    "First we will define a connection pointing to the S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5de58f6-b561-45c3-bc96-38523fcff58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE\n",
    "OR REPLACE CONNECTION AI_LAB_FIRST_STEPS_S3 TO 'https://ai-lab-example-data-s3.s3.eu-central-1.amazonaws.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23507a42-9459-4446-af7f-7d4f65294242",
   "metadata": {},
   "source": [
    "Alternatively the connection can also use the following URL syntax, see also \"_Load data from Parquet files_\" on [docs.exasol.com](https://docs.exasol.com/db/latest/loading_data/load_data_parquet.htm#Overview):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945dc2dd-8722-4455-b77e-cf6ad88eea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE\n",
    "OR REPLACE CONNECTION AI_LAB_FIRST_STEPS_S3 TO 's3://ai-lab-example-data-s3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118dd1a-40e6-412a-8624-314e7de1f893",
   "metadata": {},
   "source": [
    "Then we will remove the data imported before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba8cad-8451-4f7a-8dec-3d04950d88b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "TRUNCATE TABLE US_FLIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc80445-e5c0-43c1-ae34-f16326d6fb94",
   "metadata": {},
   "source": [
    "Now we can import the Parquet file from S3 into the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa97292-b0e5-441b-a961-a3d1fba6d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "IMPORT INTO US_FLIGHTS FROM PARQUET AT AI_LAB_FIRST_STEPS_S3 \n",
    "    FILE 'first_steps/US_FLIGHTS_FEB_2024.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074a883-ae21-479c-b4fa-17962afe8149",
   "metadata": {},
   "source": [
    "We will query table `US_FLIGHTS` again to display the imported data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064158a4-8d37-42ab-905f-e13d6c2af3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql SELECT * FROM US_FLIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582ac55-9c56-47d9-b286-948835d976c4",
   "metadata": {},
   "source": [
    "## 3. PyExasol\n",
    "\n",
    "Please note\n",
    "* Accessing Exasol database versions `2025` and higher requires Pyexasol version â‰¥ `1.2`.\n",
    "* AI Lab currently is shipped with pyexasol version `0.27.0`, the Pyexasol examples can only be executed with Exasol database versions < `2025`.\n",
    "\n",
    "### 3.1 Importing a CSV File From the Local Filesystem\n",
    "\n",
    "This section demonstrates how to import a CSV file from the local file system into the database using pyexasol.\n",
    "\n",
    "Function `open_pyexasol_connection()` opens a connection, using the configuration from the SCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40546944-dd3e-4330-a9c0-8ef57c4b5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from exasol.nb_connector.connections import open_pyexasol_connection\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    path = Path(\"first_steps/pyexasol.csv\")\n",
    "    import_params = {\n",
    "        \"column_delimiter\": '\"',\n",
    "        \"column_separator\": \",\",\n",
    "        \"row_separator\": \"CRLF\",\n",
    "        \"skip\": 1,\n",
    "    }\n",
    "    conn.import_from_file(path, (ai_lab_config.db_schema, \"US_AIRLINES\"), import_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023d10d-ccdb-4c2d-8fa5-655e227e8c76",
   "metadata": {},
   "source": [
    "Let's verify successful import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773a861-8399-4a17-9fe7-d1ac2e960f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM US_AIRLINES WHERE CARRIER_NAME LIKE '% local CSV file via pyexasol'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c96353d-4431-4cf3-99d2-0e5356721886",
   "metadata": {},
   "source": [
    "### 3.2 Importing a CSV File From Remote\n",
    "\n",
    "This section demonstrates how to import a CSV file from a remote source into the database using pyexasol.\n",
    "\n",
    "Then we will truncate table `US_FLIGHTS` to be able to import the flight data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fbedb-e070-4821-a8df-00cc964075d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "TRUNCATE TABLE US_FLIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324056a-65ad-4bfa-b044-d41154547d85",
   "metadata": {},
   "source": [
    "Now let's run the import via pyexasol. This example uses a query with separate params and [pyexasol SQL formatting](https://exasol.github.io/pyexasol/master/user_guide/exploring_features/formatting_sql.html): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04364f78-a1f2-49ec-9ec9-959a2604a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exasol.nb_connector.connections import open_pyexasol_connection\n",
    "\n",
    "query = \"\"\"\n",
    "    IMPORT INTO {flights_table!q} FROM CSV AT {url!s} FILE {file!s}\n",
    "    (1 FORMAT = {date_format!s}, 2..21) \n",
    "    SKIP = 1\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"flights_table\": (ai_lab_config.db_schema, \"US_FLIGHTS\"),\n",
    "    \"url\": \"https://dut5tonqye28.cloudfront.net/ai_lab/first_steps/\",\n",
    "    \"file\": \"US_FLIGHTS_FEB_2024-fixed-booleans.csv\",\n",
    "    \"date_format\": \"MM/DD/YYYY HH12:MI:SS AM\",\n",
    "}\n",
    "\n",
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    result = conn.execute(query, params)\n",
    "\n",
    "print(f\"Imported {result.rowcount()} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011de876-d82f-4c3b-8358-dc81c3faf601",
   "metadata": {},
   "source": [
    "We will assign the formatted SQL query to a variable for reusing it later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f63d51d-e044-4dae-ac8e-c5f9274afa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_remote_csv_sql = result.query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b8024f-69c1-4247-ab5a-946d5e98fe2b",
   "metadata": {},
   "source": [
    "## 4. SQLAlchemy\n",
    "\n",
    "Function `open_sqlalchemy_connection()` returns a SQLAlchemy engine, again using the configuration from the SCS.\n",
    "This engine will be used by the examples based on SQLAlchemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a9139-f8e1-45bd-8ea8-43571d2ee3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exasol.nb_connector.connections import open_sqlalchemy_connection\n",
    "engine = open_sqlalchemy_connection(ai_lab_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7e497-cf0a-4158-b656-221a80086a90",
   "metadata": {},
   "source": [
    "### 3.1 Importing a CSV File From Remote\n",
    "\n",
    "This section demonstrates how to import a CSV file from a remote source into the database using SQLAlchemy.\n",
    "\n",
    "First we will truncate table `US_FLIGHTS` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b831c676-b307-4af0-9b55-0ad563cfdeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "TRUNCATE TABLE US_FLIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d228759-c857-4b28-8b93-ca9baae3bc54",
   "metadata": {},
   "source": [
    "Next we will import the flight data once again, now using SQLAlchemy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce8489-e920-43ce-a9fc-3aee01ba6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusing variable import_remote_csv_sql defined in pyexasol example above\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(import_remote_csv_sql)\n",
    "print(f\"Imported {result.rowcount} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2228c1a-b6ce-432f-b869-ad18abf04b93",
   "metadata": {},
   "source": [
    "### 3.3 Importing a Parquet File from an AWS S3 Bucket\n",
    "\n",
    "This section demonstrates how to import a CSV file from an AWS S3 Bucket into the database using SQLAlchemy.\n",
    "\n",
    "**Please note**: Parquet import requires using **<span style=\"color: #40a\">Exasol version 2025 or higher</span>**, see [docs.exasol.com](https://docs.exasol.com/db/latest/loading_data/load_data_parquet.htm).\n",
    "\n",
    "First we will truncate table `US_FLIGHTS` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a19429-89b7-4ed2-b401-e1c2563c2940",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "TRUNCATE TABLE US_FLIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11dcb6d-d3e5-4e1e-8d61-ba59897c56e9",
   "metadata": {},
   "source": [
    "Now we can import the Parquet file from S3 into the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651b41c6-3cd1-4311-a5f3-ababdc9a4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "IMPORT INTO US_FLIGHTS\n",
    "    FROM PARQUET AT AI_LAB_FIRST_STEPS_S3\n",
    "    FILE 'first_steps/US_FLIGHTS_FEB_2024.parquet'\n",
    "\"\"\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(sql)\n",
    "print(f\"Imported {result.rowcount} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35335a66-dace-4082-ba35-a3bcc1ed3d54",
   "metadata": {},
   "source": [
    "### 3.4 SQLAlchemy Query Builders\n",
    "\n",
    "This section demonstrates using SQLAlchemy features [text expression](https://docs.sqlalchemy.org/en/20/core/sqlelement.html#sqlalchemy.sql.expression.text) and [TextClause](https://docs.sqlalchemy.org/en/20/core/sqlelement.html#sqlalchemy.sql.expression.TextClause) to build a query, execute it and iterate the result set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae1fad-4811-4715-a4f5-c5d1c850efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "from datetime import datetime\n",
    "from sqlalchemy.types import DateTime, String\n",
    "\n",
    "t = (\n",
    "    text(\n",
    "        \"\"\"\n",
    "        SELECT FL_DATE, CRS_DEP_TIME \n",
    "        FROM US_FLIGHTS \n",
    "        WHERE OP_CARRIER_AIRLINE_ID =:carrier_id\n",
    "        AND ORIGIN_STATE_ABR=:origin\n",
    "        AND DEST_STATE_ABR=:dest\n",
    "        \"\"\"\n",
    "    )\n",
    "    .bindparams(carrier_id=20452, origin=\"TX\", dest=\"NJ\")\n",
    "    .columns(FL_DATE=DateTime, CRS_DEP_TIME=String)\n",
    ")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    for dt, departure in conn.execute(t):\n",
    "        print(datetime.fromisoformat(dt).date(), departure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ca230-817c-4226-a1b6-ac5c32d4b1c7",
   "metadata": {},
   "source": [
    "## 5. Using the Exasol Bucket File System\n",
    "\n",
    "The [Exasol Bucket File System](https://docs.exasol.com/db/latest/database_concepts/bucketfs/bucketfs.htm) (BucketFS) is a powerful feature for exchanging non-relational data with the database nodes in an Exasol cluster.\n",
    "\n",
    "Such data can be arbitrary files including \n",
    "* Data to be processed by [User Defined Scripts](https://docs.exasol.com/db/latest/database_concepts/udf_scripts.htm) (UDFs)\n",
    "* [Script-Language Containers](https://github.com/exasol/script-languages-release) (SLCs)\n",
    "* Pretrained Large Language AI Models\n",
    "\n",
    "### 5.1 Uploading a File to the BucketFS\n",
    "\n",
    "First we will create a sample file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da84a6-c1bd-4159-830e-88a2fa94ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile first_steps/text_file.txt\n",
    "Hello World!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644c5311-27e4-463c-b228-c44ce810b50e",
   "metadata": {},
   "source": [
    "And now, let's upload the file into the BucketFS.\n",
    "\n",
    "Function `open_bucketfs_location()` returns a cursor into Exasols BucketFS, also using the configuration in the SCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ab083-d7db-4e76-8e1d-7cf7ff369c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exasol.nb_connector.connections import open_bucketfs_location\n",
    "from pathlib import Path \n",
    "\n",
    "file = Path(\"first_steps/text_file.txt\")\n",
    "bfs = open_bucketfs_location(ai_lab_config)\n",
    "remote = bfs / file.name\n",
    "remote.write(file.read_bytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc65f7-4d27-41b2-835a-100aa8b79d64",
   "metadata": {},
   "source": [
    "### 5.2 Listing the Files in the BucketFS\n",
    "\n",
    "We can also list all the files currently available in the BucketFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597aa88-5c37-4d2d-ae96-be90bf1a5baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfs = open_bucketfs_location(ai_lab_config)\n",
    "for p in bfs.iterdir():\n",
    "    print(f'- {p.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ff5dd-cf57-4b51-aeff-26bf1d879a85",
   "metadata": {},
   "source": [
    "### 5.3 Reading a File in the BucketFS\n",
    "\n",
    "We can also read the contents of a file in the BucketFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d3b55-04d1-4fc1-a307-383a64eed012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import exasol.bucketfs as bfs\n",
    "\n",
    "content = bfs.as_string(remote.read())\n",
    "print(f'The file in the BucketFS contains:\\n{content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b22225-34de-435b-b29d-6712e960b926",
   "metadata": {},
   "source": [
    "### 5.4 Reading the File in the BucketFS Using a User Defined Function (UDF)\n",
    "\n",
    "[TODO]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
