{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Advanced UDF with GPU\n",
    "\n",
    "This perform an in-depth analysis of news articles using the data provided in the S3 virtual schema tutorial. The dataset that we will be working with, is derived from the well-known Reuters 21578 collection—a comprehensive repository of documents containing a wide variety of news articles, each categorized according to its subject matter.\n",
    "\n",
    "Our first step will be to create two distinct columns:\n",
    "1. Column **Topics:** will contain unique topics present in each news article, allowing us to systematically analyze the subject areas covered by the news article.\n",
    "2. Column **Titles:** will contain the headline or title of each news article, which we will later use for semantic matching.\n",
    "\n",
    "Once our tables are set up and populated, we will utilize a powerful natural language processing approach to link news article titles to their most relevant topics. Specifically, we will employ the `sentence-transformers` model—a state-of-the-art model designed for producing high-quality sentence embeddings. This model will run in a UDF leveraging the GPU to efficiently generate vector representations for both the titles and topics.\n",
    "\n",
    "By comparing these embeddings, our system will automatically match each article title with the most relevant topic from our topics table. Finally, we will output the best-matching topic for each title, providing valuable insights into the subject distribution of our set of news articles."
   ],
   "id": "3099daff8bb6b53f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Open Secure Configuration Storage",
   "id": "50a2988007bec846"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%run ../utils/access_store_ui.ipynb\n",
    "display(get_access_store_ui('../'))"
   ],
   "id": "cb54534177a867be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prerequisites\n",
    "- Run the [S3 VS tutorial](../cloud/02_s3_vs_reuters.ipynb) in order to populate our input data"
   ],
   "id": "73a7e60d9fe78077"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Instantiate ScriptLanguagesContainer\n",
    "\n",
    "The following cell creates an instance of class `ScriptLanguageContainer` from the notebook-connector,\n",
    "which enables using the`exaslct` in the AI Lab in a convenient way."
   ],
   "id": "fd5b8ce9f2bc2326"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from exasol.nb_connector.slc import ScriptLanguageContainer\n",
    "slc = ScriptLanguageContainer(secrets=ai_lab_config, name=\"gpu_slc\")"
   ],
   "id": "86d94d7e56b6e3da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Connect to the database and activate the container",
   "id": "3591c8128d27776"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from exasol.nb_connector.language_container_activation import open_pyexasol_connection_with_lang_definitions\n",
    "\n",
    "conn = open_pyexasol_connection_with_lang_definitions(ai_lab_config, schema=ai_lab_config.db_schema, compression=True)"
   ],
   "id": "d1b725da3391679d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prepare the models",
   "id": "5f87d10ed6cf0835"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from exasol.nb_connector.model_installation import install_model, TransformerModel\n",
    "from transformers import AutoModel"
   ],
   "id": "7e71b1278aa02213"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "install_model(ai_lab_config, TransformerModel(\n",
    "                \"sentence-transformers/all-mpnet-base-v2\", \"feature-extraction\", AutoModel\n",
    "            ),)"
   ],
   "id": "ea370394b0492712"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create the UDF\n",
    "\n",
    "Now create the UDF which implements a \"semantic join\" between pairs of text data (text1, text2) by computing their semantic similarity using a transformer neural network model (from the sentence-transformers library). It runs on a GPU and is designed to scale to large amounts of data by processing in batches. For each pair (text1 and text2), computes the cosine similarity between their embeddings in a row-wise fashion, on the GPU. The UDF returns (emits) text1 and text2, as well as the `similarity_score` (between -1 and 1, with 1.0 = identical).\n"
   ],
   "id": "67d0dc114de6c50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sql_semantic_join = f\"\"\"\n",
    "--/\n",
    "CREATE OR REPLACE {slc.language_alias} SET SCRIPT \"SEMANTIC_JOIN\"(text1 VARCHAR(2000000), text2 VARCHAR(2000000))\n",
    "EMITS(text1 VARCHAR(2000000), text2 VARCHAR(2000000), similarity_score DOUBLE) AS\n",
    "%perInstanceRequiredAcceleratorDevices GpuNvidia;\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, Pipeline, pipeline\n",
    "import torch\n",
    "import exasol.bucketfs as bfs\n",
    "from pathlib import Path\n",
    "from exasol_transformers_extension.utils import device_management\n",
    "from exasol.python_extension_common.connections.bucketfs_location import (\n",
    "    create_bucketfs_location_from_conn_object)\n",
    "from exasol_transformers_extension.utils.bucketfs_model_specification import (\n",
    "    BucketFSModelSpecification)\n",
    "from exasol_transformers_extension.utils.huggingface_hub_bucketfs_model_transfer_sp import (\n",
    "    HuggingFaceHubBucketFSModelTransferSP)\n",
    "from exasol_transformers_extension.utils.load_local_model import LoadLocalModel\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def get_bucketfs_location(exa, bucketfs_conn_name: str) -> bfs.path.PathLike:\n",
    "    return create_bucketfs_location_from_conn_object(\n",
    "        exa.get_connection(bucketfs_conn_name))\n",
    "\n",
    "def load_transformers_pipline(exa,\n",
    "                              bucketfs_conn_name: str,\n",
    "                              sub_dir: str,\n",
    "                              device: str,\n",
    "                              task_type: str,\n",
    "                              model_name: str,\n",
    "                              model_factory,\n",
    "                              tokenizer_factory=AutoTokenizer) -> Pipeline:\n",
    "    model_loader = LoadLocalModel(pipeline,\n",
    "                                  base_model_factory=model_factory,\n",
    "                                  tokenizer_factory=tokenizer_factory,  # type: ignore\n",
    "                                  task_type=task_type,\n",
    "                                  device=device)    # type: ignore\n",
    "\n",
    "    model_spec = BucketFSModelSpecification(model_name, task_type, bucketfs_conn_name,\n",
    "                                            Path(sub_dir))\n",
    "\n",
    "    bucketfs_location = get_bucketfs_location(exa, bucketfs_conn_name)\n",
    "\n",
    "    model_loader.clear_device_memory()\n",
    "    model_loader.set_current_model_specification(model_spec)\n",
    "    model_loader.set_bucketfs_model_cache_dir(bucketfs_location)\n",
    "    return model_loader.load_models()\n",
    "\n",
    "model_pipeline = load_transformers_pipline(\n",
    "    exa,\n",
    "    bucketfs_conn_name='{ai_lab_config.bfs_connection_name}',\n",
    "    sub_dir='{ai_lab_config.bfs_model_subdir}',\n",
    "    device=device,\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    task_type='feature-extraction',\n",
    "    model_factory=AutoModel)\n",
    "\n",
    "# Function to calculate embeddings\n",
    "def batch_get_embeddings(batch_texts, model_pipeline, device):\n",
    "    # Tokenize the batch of texts\n",
    "    inputs = model_pipeline.tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt').to(device)  # Send to GPU\n",
    "    # Get embeddings from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model_pipeline.model(**inputs)\n",
    "    # Use the outputs pooler_output or last_hidden_state to get the embedding\n",
    "    return outputs.pooler_output if 'pooler_output' in outputs else outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Function to compute row-by-row cosine similarity using PyTorch\n",
    "def row_by_row_cosine_similarity_gpu(embeddings1, embeddings2):\n",
    "    # Normalized embeddings can be calculated separately\n",
    "    normalized_embeddings1 = embeddings1 / embeddings1.norm(dim=1)[:, None]\n",
    "    normalized_embeddings2 = embeddings2 / embeddings2.norm(dim=1)[:, None]\n",
    "\n",
    "    # Compute pair-wise cosine similarity for each corresponding pair\n",
    "    similarities = (normalized_embeddings1 * normalized_embeddings2).sum(dim=1)\n",
    "    return similarities\n",
    "\n",
    "def run(ctx):\n",
    "    # Batch processing configurations\n",
    "    batch_size = 100  # Adjust this based on your memory capacity\n",
    "\n",
    "    while True:\n",
    "        df = ctx.get_dataframe(batch_size)\n",
    "        if df is None:\n",
    "            break\n",
    "\n",
    "        data1 = df[\"text1\"]\n",
    "        data2 = df[\"text2\"]\n",
    "\n",
    "        # Compute embeddings for each dataset in batches\n",
    "        embeddings1 = batch_get_embeddings(data1.to_list(), model_pipeline, device)\n",
    "        embeddings2 = batch_get_embeddings(data2.to_list(), model_pipeline, device)\n",
    "\n",
    "        # Calculate row-by-row cosine similarity using PyTorch on GPU\n",
    "        similarity_scores = row_by_row_cosine_similarity_gpu(embeddings1, embeddings2)\n",
    "\n",
    "        # Determine matches based on similarity scores (threshold can be adjusted)\n",
    "        df[\"SIMILARITY_SCORE\"]=similarity_scores.cpu()\n",
    "        ctx.emit(df)\n",
    "/\n",
    "\"\"\"\n",
    "conn.execute(sql_semantic_join)"
   ],
   "id": "f343b288db1d4614"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Initialise JupySQL\n",
    "Let's bring up JupySQL and connect to the database via SQLAlchemy. Please refer to the documentation of <a href=\"https://github.com/exasol/sqlalchemy-exasol\" target=\"_blank\" rel=\"noopener\">sqlalchemy-exasol</a> for details on how to connect to the database using the Exasol SQLAlchemy driver."
   ],
   "id": "34c2b7fa6edb53b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%run ../utils/jupysql_init.ipynb",
   "id": "21770aa932a9fb4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Run the UDF on the new data\n",
    "\n",
    "Now we apply the `semantic_join` UDF on the topics and titles from the NEWS table, which was created by the S3 Virtual Schema tutorial: for every news TITLE, the TOPICs that best semantically match it, ranks them by how similar they are, and filters out all but the strong matches (similarity > 0.5). You get, for each TITLE, a list of TOPICs it is semantically closest to, in order."
   ],
   "id": "68e6a6cadb020ea8"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql1"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT\n",
    "        TEXT1 AS TITLE, TEXT2 AS TOPIC,\n",
    "        RANK() OVER (PARTITION BY TEXT1 ORDER BY SIMILARITY_SCORE DESC) as r,\n",
    "        SIMILARITY_SCORE\n",
    "    FROM (\n",
    "        SELECT\n",
    "            \"SEMANTIC_JOIN\"(TITLE, TOPIC)\n",
    "        FROM (\n",
    "            SELECT distinct TOPIC, TITLE\n",
    "            FROM (\n",
    "                SELECT distinct title as TITLE FROM NEWS WHERE TITLE IS NOT NULL AND BODY IS NOT NULL\n",
    "            )\n",
    "            CROSS JOIN (\n",
    "                SELECT distinct topic AS TOPIC FROM NEWS WHERE topic IS NOT NULL\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    WHERE SIMILARITY_SCORE > 0.5\n",
    ")\n",
    "WHERE r<=5\n",
    "ORDER BY TITLE, r ASC"
   ],
   "id": "a9886b4638692671"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Additional Notes\n",
    "\n",
    "#### Parallel execution\n",
    "Please note that executing queries with GPU UDFs in parallel can lead to unexpected errors, as the GPU libraries used within the UDFs may attempt to allocate the same GPU device and memory simultaneously. In the current Exasol version, it's the users responsibility to ensure that the libraries are configured appropriately or that only one query uses a GPU devices.\n",
    "Here some links which might be helpful for the configuration:\n",
    "- [Manage GPU Memory When Using TensorFlow and PyTorch](https://docs.ncsa.illinois.edu/systems/hal/en/latest/user-guide/prog-env/gpu-memory.html)\n",
    "- [pytorch set per process memory fraction](https://docs.pytorch.org/docs/stable/generated/torch.cuda.memory.set_per_process_memory_fraction.html)\n",
    "- [TensorFlow LogicalDeviceConfiguration](https://www.tensorflow.org/api_docs/python/tf/config/LogicalDeviceConfiguration)\n",
    "\n",
    "Additionally, it is possible to control the number of parallel UDF instances by setting the  [UDF Instance Limiting](https://docs.exasol.com/db/latest/database_concepts/udf_scripts/udf_instance_limit.htm).\n"
   ],
   "id": "2e1d00d1b4c3ea0f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
