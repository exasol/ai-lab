{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3099daff8bb6b53f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/exasol/ai-lab/refs/heads/main/assets/Exasol_Logo_2025_Dark.svg\" style=\"width:200px; margin: 10px;\" />\n",
    "</div>\n",
    "\n",
    "## Advanced UDF using GPU Acceleration\n",
    "\n",
    "This perform an in-depth analysis of news articles using the data provided in the S3 virtual schema tutorial. The dataset that we will be working with, is derived from the well-known Reuters 21578 collection—a comprehensive repository of documents containing a wide variety of news articles, each categorized according to its subject matter.\n",
    "\n",
    "Our first step will be to create two distinct columns:\n",
    "1. Column **Topics:** will contain unique topics present in each news article, allowing us to systematically analyze the subject areas covered by the news article.\n",
    "2. Column **Titles:** will contain the headline or title of each news article, which we will later use for semantic matching.\n",
    "\n",
    "Once our tables are set up and populated, we will utilize a powerful natural language processing approach to link news article titles to their most relevant topics. Specifically, we will employ the `sentence-transformers` model—a state-of-the-art model designed for producing high-quality sentence embeddings. This model will run in a UDF leveraging the GPU to efficiently generate vector representations for both the titles and topics.\n",
    "\n",
    "By comparing these embeddings, our system will automatically match each article title with the most relevant topic from our topics table. Finally, we will output the top-N topics for each title, providing valuable insights into the subject distribution of our set of news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a2988007bec846",
   "metadata": {},
   "source": [
    "### Open Secure Configuration Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb54534177a867be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be885525004d4fa4a70014fade69042d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d1ec7fb427480aba1fdf28aacdd8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Box(children=(Label(value='Configuration Store', layout=Layout(border_bottom='solid 1px', border…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../utils/access_store_ui.ipynb\n",
    "display(get_access_store_ui('../'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7e60d9fe78077",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "- Run the [S3 VS tutorial](../cloud/02_s3_vs_reuters.ipynb) in order to populate the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5b8ce9f2bc2326",
   "metadata": {},
   "source": [
    "### Instantiate ScriptLanguagesContainer\n",
    "\n",
    "The following cell creates an instance of class `ScriptLanguageContainer` from the notebook-connector,\n",
    "which enables us to use a custom Script Language Container (SLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d94d7e56b6e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exasol.nb_connector.slc import ScriptLanguageContainer\n",
    "slc = ScriptLanguageContainer(secrets=ai_lab_config, name=\"gpu_slc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3591c8128d27776",
   "metadata": {},
   "source": [
    "### Connect to the Database and activate the Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b725da3391679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exasol.nb_connector.language_container_activation import open_pyexasol_connection_with_lang_definitions\n",
    "\n",
    "conn = open_pyexasol_connection_with_lang_definitions(ai_lab_config, schema=ai_lab_config.db_schema, compression=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87d10ed6cf0835",
   "metadata": {},
   "source": [
    "### Prepare the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e71b1278aa02213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exasol.nb_connector.model_installation import install_model, TransformerModel\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea370394b0492712",
   "metadata": {
    "tags": [
     "uploading_model"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠙ - Huggingface model sentence-transformers/all-mpnet-base-v2 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/jupyterenv/lib/python3.10/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ec2-54-246-49-57.eu-west-1.compute.amazonaws.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠸ - Huggingface model sentence-transformers/all-mpnet-base-v2 "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6b6bd7c3024e0ea9fa5a892def9e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠇ - Huggingface model sentence-transformers/all-mpnet-base-v2 "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd20ce8a05374aa0abe514bda8b1dbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠴ - Huggingface model sentence-transformers/all-mpnet-base-v2 "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8334b2dd7394ea5ae90bdefe76fb77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠹ - Huggingface model sentence-transformers/all-mpnet-base-v2 "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b042759a63647fda8b53039a789fb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠴ - Huggingface model sentence-transformers/all-mpnet-base-v2 "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03b49c8c8ca4e65975ff529da963882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠋ - Huggingface model sentence-transformers/all-mpnet-base-v2 "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4d7589905a4b3884e67782b3fa64dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⠏ - Huggingface model sentence-transformers/all-mpnet-base-v2 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/jupyterenv/lib/python3.10/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ec2-54-246-49-57.eu-west-1.compute.amazonaws.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ - Huggingface model sentence-transformers/all-mpnet-base-v2 \n"
     ]
    }
   ],
   "source": [
    "install_model(ai_lab_config, TransformerModel(\n",
    "                \"sentence-transformers/all-mpnet-base-v2\", \"feature-extraction\", AutoModel\n",
    "            ),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d0dc114de6c50",
   "metadata": {},
   "source": [
    "### Create the UDF\n",
    "\n",
    "Now we are ready to create the UDF implementing the \"semantic join\" between pairs of text data (_text1_, _text2_) by computing their semantic similarity using a transformer neural network model (from the sentence-transformers library). It runs on a GPU and is designed to scale to large amounts of data by processing in batches. For each pair (_text1_, _text2_), it computes the cosine similarity between their embeddings on the GPU. The UDF returns (emits) _text1_ and _text2_, as well as the `similarity_score` (between -1 and 1, with 1.0 = identical).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f343b288db1d4614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ExaStatement session_id=1850020851146555392 stmt_idx=2>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_semantic_join = f\"\"\"\n",
    "--/\n",
    "CREATE OR REPLACE {slc.language_alias} SET SCRIPT \"SEMANTIC_JOIN\"(text1 VARCHAR(2000000), text2 VARCHAR(2000000))\n",
    "EMITS(text1 VARCHAR(2000000), text2 VARCHAR(2000000), similarity_score DOUBLE) AS\n",
    "%perInstanceRequiredAcceleratorDevices GpuNvidia;\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, Pipeline, pipeline\n",
    "import torch\n",
    "import exasol.bucketfs as bfs\n",
    "from pathlib import Path\n",
    "from exasol_transformers_extension.utils import device_management\n",
    "from exasol.python_extension_common.connections.bucketfs_location import (\n",
    "    create_bucketfs_location_from_conn_object)\n",
    "from exasol_transformers_extension.utils.bucketfs_model_specification import (\n",
    "    BucketFSModelSpecification)\n",
    "from exasol_transformers_extension.utils.huggingface_hub_bucketfs_model_transfer_sp import (\n",
    "    HuggingFaceHubBucketFSModelTransferSP)\n",
    "from exasol_transformers_extension.utils.load_local_model import LoadLocalModel\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def get_bucketfs_location(exa, bucketfs_conn_name: str) -> bfs.path.PathLike:\n",
    "    return create_bucketfs_location_from_conn_object(\n",
    "        exa.get_connection(bucketfs_conn_name))\n",
    "\n",
    "def load_transformers_pipline(exa,\n",
    "                              bucketfs_conn_name: str,\n",
    "                              sub_dir: str,\n",
    "                              device: str,\n",
    "                              task_type: str,\n",
    "                              model_name: str,\n",
    "                              model_factory,\n",
    "                              tokenizer_factory=AutoTokenizer) -> Pipeline:\n",
    "    model_loader = LoadLocalModel(pipeline,\n",
    "                                  base_model_factory=model_factory,\n",
    "                                  tokenizer_factory=tokenizer_factory,  # type: ignore\n",
    "                                  task_type=task_type,\n",
    "                                  device=device)    # type: ignore\n",
    "\n",
    "    model_spec = BucketFSModelSpecification(model_name, task_type, bucketfs_conn_name,\n",
    "                                            Path(sub_dir))\n",
    "\n",
    "    bucketfs_location = get_bucketfs_location(exa, bucketfs_conn_name)\n",
    "\n",
    "    model_loader.clear_device_memory()\n",
    "    model_loader.set_current_model_specification(model_spec)\n",
    "    model_loader.set_bucketfs_model_cache_dir(bucketfs_location)\n",
    "    return model_loader.load_models()\n",
    "\n",
    "model_pipeline = load_transformers_pipline(\n",
    "    exa,\n",
    "    bucketfs_conn_name='{ai_lab_config.bfs_connection_name}',\n",
    "    sub_dir='{ai_lab_config.bfs_model_subdir}',\n",
    "    device=device,\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    task_type='feature-extraction',\n",
    "    model_factory=AutoModel)\n",
    "\n",
    "# Function to calculate embeddings\n",
    "def batch_get_embeddings(batch_texts, model_pipeline, device):\n",
    "    # Tokenize the batch of texts\n",
    "    inputs = model_pipeline.tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt').to(device)  # Send to GPU\n",
    "    # Get embeddings from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model_pipeline.model(**inputs)\n",
    "    # Use the outputs pooler_output or last_hidden_state to get the embedding\n",
    "    return outputs.pooler_output if 'pooler_output' in outputs else outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Function to compute row-by-row cosine similarity using PyTorch\n",
    "def row_by_row_cosine_similarity_gpu(embeddings1, embeddings2):\n",
    "    # Normalized embeddings can be calculated separately\n",
    "    normalized_embeddings1 = embeddings1 / embeddings1.norm(dim=1)[:, None]\n",
    "    normalized_embeddings2 = embeddings2 / embeddings2.norm(dim=1)[:, None]\n",
    "\n",
    "    # Compute pair-wise cosine similarity for each corresponding pair\n",
    "    similarities = (normalized_embeddings1 * normalized_embeddings2).sum(dim=1)\n",
    "    return similarities\n",
    "\n",
    "def run(ctx):\n",
    "    # Batch processing configurations\n",
    "    batch_size = 100  # Adjust this based on your memory capacity\n",
    "\n",
    "    while True:\n",
    "        df = ctx.get_dataframe(batch_size)\n",
    "        if df is None:\n",
    "            break\n",
    "\n",
    "        data1 = df[\"text1\"]\n",
    "        data2 = df[\"text2\"]\n",
    "\n",
    "        # Compute embeddings for each dataset in batches\n",
    "        embeddings1 = batch_get_embeddings(data1.to_list(), model_pipeline, device)\n",
    "        embeddings2 = batch_get_embeddings(data2.to_list(), model_pipeline, device)\n",
    "\n",
    "        # Calculate row-by-row cosine similarity using PyTorch on GPU\n",
    "        similarity_scores = row_by_row_cosine_similarity_gpu(embeddings1, embeddings2)\n",
    "\n",
    "        # Determine matches based on similarity scores (threshold can be adjusted)\n",
    "        df[\"SIMILARITY_SCORE\"]=similarity_scores.cpu()\n",
    "        ctx.emit(df)\n",
    "/\n",
    "\"\"\"\n",
    "conn.execute(sql_semantic_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2b7fa6edb53b5",
   "metadata": {},
   "source": [
    "### Initialize JupySQL\n",
    "Let's bring up JupySQL and connect to the database via SQLAlchemy. Please refer to the documentation of <a href=\"https://github.com/exasol/sqlalchemy-exasol\" target=\"_blank\" rel=\"noopener\">sqlalchemy-exasol</a> for details on how to connect to the database using the Exasol SQLAlchemy driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21770aa932a9fb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;exa+websocket://sys:***@ec2-54-246-49-57.eu-west-1.compute.amazonaws.com:8563/AI_LAB?ENCRYPTION=Yes&amp;SSLCertificate=SSL_VERIFY_NONE&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'exa+websocket://sys:***@ec2-54-246-49-57.eu-west-1.compute.amazonaws.com:8563/AI_LAB?ENCRYPTION=Yes&SSLCertificate=SSL_VERIFY_NONE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;exa+websocket://sys:***@ec2-54-246-49-57.eu-west-1.compute.amazonaws.com:8563/AI_LAB?ENCRYPTION=Yes&amp;SSLCertificate=SSL_VERIFY_NONE&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'exa+websocket://sys:***@ec2-54-246-49-57.eu-west-1.compute.amazonaws.com:8563/AI_LAB?ENCRYPTION=Yes&SSLCertificate=SSL_VERIFY_NONE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../utils/jupysql_init.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e6a6cadb020ea8",
   "metadata": {},
   "source": [
    "### Run the UDF on the New Data\n",
    "\n",
    "Now we apply the `semantic_join` UDF on the topics and titles from the NEWS table, which was created by the S3 Virtual Schema tutorial. The query will return for every news TITLE, the TOPICs that best semantically match it, ranks them by how similar they are, and filters out all but the strong matches (similarity > 0.5). You get, for each TITLE, a list of most plausible TOPICs, in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9886b4638692671",
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"None\">Running query in &#x27;exa+websocket://sys:***@ec2-54-246-49-57.eu-west-1.compute.amazonaws.com:8563/AI_LAB?ENCRYPTION=Yes&amp;SSLCertificate=SSL_VERIFY_NONE&#x27;</span>"
      ],
      "text/plain": [
       "Running query in 'exa+websocket://sys:***@ec2-54-246-49-57.eu-west-1.compute.amazonaws.com:8563/AI_LAB?ENCRYPTION=Yes&SSLCertificate=SSL_VERIFY_NONE'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color: green\">12 rows affected.</span>"
      ],
      "text/plain": [
       "12 rows affected."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>title</th>\n",
       "            <th>topic</th>\n",
       "            <th>r</th>\n",
       "            <th>similarity_score</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>(RPT) U.S. SAYS TIN DISPOSALS WILL NOT AFFECT ACCORD</td>\n",
       "            <td>tin</td>\n",
       "            <td>1</td>\n",
       "            <td>0.5479601621627808</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>ATPC MEMBERS FIND WAYS TO CURB TIN EXPORTS</td>\n",
       "            <td>tin</td>\n",
       "            <td>1</td>\n",
       "            <td>0.602620005607605</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>BAHIA COCOA REVIEW</td>\n",
       "            <td>cocoa</td>\n",
       "            <td>1</td>\n",
       "            <td>0.6362672448158264</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>CANADA RULING ON U.S. CORN INJURY DUE THIS WEEK</td>\n",
       "            <td>corn</td>\n",
       "            <td>1</td>\n",
       "            <td>0.5086946487426758</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>CHINESE WHEAT CROP THREATENED BY PESTS, DISEASE</td>\n",
       "            <td>wheat</td>\n",
       "            <td>1</td>\n",
       "            <td>0.5508740544319153</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>INDONESIAN TEA, COCOA EXPORTS SEEN UP, COFFEE DOWN</td>\n",
       "            <td>cocoa</td>\n",
       "            <td>1</td>\n",
       "            <td>0.5876307487487793</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>NYMEX TO SUBMIT PROPANE PROPOSAL TO CFTC</td>\n",
       "            <td>propane</td>\n",
       "            <td>1</td>\n",
       "            <td>0.5077935457229614</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>OFFICIAL INQUIRY SET FOR AUSTRALIAN WHEAT INDUSTRY</td>\n",
       "            <td>wheat</td>\n",
       "            <td>1</td>\n",
       "            <td>0.5389493703842163</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>STOP AND SHOP COS INC <SHP> 4TH QTR JAN 31 NET</td>\n",
       "            <td>retail</td>\n",
       "            <td>1</td>\n",
       "            <td>0.5028800964355469</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>U.S. WEEKLY SOYBEAN CRUSH 21,782,929 BUSHELS</td>\n",
       "            <td>soybean</td>\n",
       "            <td>1</td>\n",
       "            <td>0.5226584672927856</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<span style=\"font-style:italic;text-align:center;\">Truncated to <a href=\"https://jupysql.ploomber.io/en/latest/api/configuration.html#displaylimit\">displaylimit</a> of 10.</span>"
      ],
      "text/plain": [
       "+------------------------------------------------------+---------+---+--------------------+\n",
       "|                        title                         |  topic  | r |  similarity_score  |\n",
       "+------------------------------------------------------+---------+---+--------------------+\n",
       "| (RPT) U.S. SAYS TIN DISPOSALS WILL NOT AFFECT ACCORD |   tin   | 1 | 0.5479601621627808 |\n",
       "|      ATPC MEMBERS FIND WAYS TO CURB TIN EXPORTS      |   tin   | 1 | 0.602620005607605  |\n",
       "|                  BAHIA COCOA REVIEW                  |  cocoa  | 1 | 0.6362672448158264 |\n",
       "|   CANADA RULING ON U.S. CORN INJURY DUE THIS WEEK    |   corn  | 1 | 0.5086946487426758 |\n",
       "|   CHINESE WHEAT CROP THREATENED BY PESTS, DISEASE    |  wheat  | 1 | 0.5508740544319153 |\n",
       "|  INDONESIAN TEA, COCOA EXPORTS SEEN UP, COFFEE DOWN  |  cocoa  | 1 | 0.5876307487487793 |\n",
       "|       NYMEX TO SUBMIT PROPANE PROPOSAL TO CFTC       | propane | 1 | 0.5077935457229614 |\n",
       "|  OFFICIAL INQUIRY SET FOR AUSTRALIAN WHEAT INDUSTRY  |  wheat  | 1 | 0.5389493703842163 |\n",
       "|    STOP AND SHOP COS INC <SHP> 4TH QTR JAN 31 NET    |  retail | 1 | 0.5028800964355469 |\n",
       "|     U.S. WEEKLY SOYBEAN CRUSH 21,782,929 BUSHELS     | soybean | 1 | 0.5226584672927856 |\n",
       "+------------------------------------------------------+---------+---+--------------------+\n",
       "Truncated to displaylimit of 10."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT\n",
    "        TEXT1 AS TITLE, TEXT2 AS TOPIC,\n",
    "        RANK() OVER (PARTITION BY TEXT1 ORDER BY SIMILARITY_SCORE DESC) as r,\n",
    "        SIMILARITY_SCORE\n",
    "    FROM (\n",
    "        SELECT\n",
    "            \"SEMANTIC_JOIN\"(TITLE, TOPIC)\n",
    "        FROM (\n",
    "            SELECT distinct TOPIC, TITLE\n",
    "            FROM (\n",
    "                SELECT distinct title as TITLE FROM NEWS WHERE TITLE IS NOT NULL AND BODY IS NOT NULL\n",
    "            )\n",
    "            CROSS JOIN (\n",
    "                SELECT distinct topic AS TOPIC FROM NEWS WHERE topic IS NOT NULL\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    WHERE SIMILARITY_SCORE > 0.5\n",
    ")\n",
    "WHERE r<=5\n",
    "ORDER BY TITLE, r ASC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1d00d1b4c3ea0f",
   "metadata": {},
   "source": [
    "### Additional Notes\n",
    "\n",
    "#### Parallel execution\n",
    "Please note that executing multiple queries with GPU UDFs in parallel can lead to unexpected errors, as the GPU libraries used within the UDFs may attempt to allocate the same GPU device and memory simultaneously. In the current Exasol version, the user responsible to ensure that the libraries are configured appropriately or that only one query uses a GPU device.\n",
    "\n",
    "Check out the [GPU Resource Considerations](./gpu_resource_considerations.ipynb) for more information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
