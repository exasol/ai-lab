{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Text AI Extension preprocessing\n",
    "\n",
    "Here we will demonstrate how the Text AI Extension data-preprocessing can be used.\n",
    "\n",
    "    Explain wich options and stuff we have, something about why do preprocessing? or is that to basic or out of scope?\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Prior to using this notebook one needs to complete the following steps:\n",
    "1. [Configure the AI-Lab](../main_config.ipynb).\n",
    "2. [initialize the Text AI Extension](./txaie_init.ipynb)\n",
    "3. [initialize the Transformers Extension](../transformers/te_init.ipynb)\n",
    "\n",
    "## Activate the Text AI Extension SLC"
   ],
   "id": "348c85d1b34a6511"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from exasol.nb_connector.connections import open_pyexasol_connection\n",
    "from exasol.nb_connector.language_container_activation import get_activation_sql\n",
    "\n",
    "activation_sql = get_activation_sql(ai_lab_config)"
   ],
   "id": "3fb31166fdb6b628"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "do we need to do this with each new connection? and therefore not here?",
   "id": "7cf99cc87b61bf13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "    conn.execute(query=activation_sql)"
   ],
   "id": "c98de7a37378b5c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "triggers preprocessing to create the text annotations and text extraction.\n",
    "\n"
   ],
   "id": "4c0cf97ed5647fe8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Get an example dataset\n",
    "\n",
    "We will be using a Dataset which holds information on customer support tickets. We will split this data into 2 set, in order to demonstrate how the preprocessing tasks handle new data being added to a data set.\n",
    "But first we want to make sure the tables we want to use don't already exist, for example from a previous run of this notebook. Therefore, we are going to drop them.\n",
    "First, we define a list of tables to drop:"
   ],
   "id": "aafa993ed130b0bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "table_list = [\n",
    "    \"TOPIC_CLASSIFIER\",\n",
    "    \"TOPIC_CLASSIFIER_LOOKUP_TOPIC\",\n",
    "    \"TOPIC_CLASSIFIER_LOOKUP_SETUP\",\n",
    "    \"NAMED_ENTITY\",\n",
    "    \"NAMED_ENTITY_LOOKUP_ENTITY_NAME\",\n",
    "    \"NAMED_ENTITY_LOOKUP_SETUP\",\n",
    "    \"DOCUMENTS\",\n",
    "    \"DOCUMENTS_AI_LAB_CUSTOMER_SUPPORT_TICKETS\",\n",
    "    \"KEYWORD_SEARCH\",\n",
    "    \"KEYWORD_SEARCH_LOOKUP_KEYWORD\",\n",
    "    \"KEYWORD_SEARCH_LOOKUP_SETUP\"\n",
    "]"
   ],
   "id": "8d6664ace3f099c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, define a function which drops these tables, as well as our main table. Then we call the function.",
   "id": "a3f5b3a3a460e693"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "table=\"MY_TABLE\"\n",
    "OUTPUT_SCHEMA=ai_lab_config.db_schema\n",
    "\n",
    "def delete_text_ai_preprocessing_tables():\n",
    "    with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "        for drop_table in table_list:\n",
    "            conn.execute(f\"\"\"DROP TABLE IF EXISTS \"{OUTPUT_SCHEMA}\".\"{drop_table}\" \"\"\")\n",
    "        conn.execute(f\"\"\"DROP TABLE IF EXISTS \"{schema}\".\"{table}\" \"\"\")"
   ],
   "id": "d20b92c58445c8ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "delete_text_ai_preprocessing_tables()",
   "id": "33f56bd4325e2a50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can then load the data using [this notebook](../data/data_customer_support.ipynb). This loads the data into a table called \"CUSTOMER_SUPPORT_TICKETS\" found in the schema defined in the ai_lab_config variable db_schema.\n",
    "For the purpose of this notebook, we want to split this data into two parts. So we need to load it into a pandas dataframe."
   ],
   "id": "a10afada75565b4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "        whole_data_df = conn.export_to_pandas(f\"\"\"SELECT * FROM \"{ai_lab_config.db_schema}\".\"CUSTOMER_SUPPORT_TICKETS\" \"\"\")"
   ],
   "id": "a775cdefdb5a4f4e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then, we split the dataframe into two separate dataframes randomly. We will upload the first one in our data table ???.",
   "id": "6488ffe2755f950c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "shuffled = df.sample(frac=1)\n",
    "split_df_list = np.array_split(shuffled, 2)\n",
    "#todo create a view with limit (100) instead, add to view for step 3 below"
   ],
   "id": "2e6328f61f350426"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Get Models\n",
    "\n",
    "we will use multiple different transformers models to run our preprocessing with. we need to download these from huggingface.\n",
    "first, we define which models we want to use. you can browse for your preferred model [here](https://huggingface.co/models)."
   ],
   "id": "3223947757258424"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "NAMED_ENTITY_MODEL=\"guishe/nuner-v2_fewnerd_fine_super\"\n",
    "NLI_MODEL=\"tasksource/ModernBERT-large-nli\"\n",
    "FEATURE_EXTRACTION_MODEL=\"answerdotai/ModernBERT-large\""
   ],
   "id": "25c7d540972c2c34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then we import the \"load_huggingface_model\" defined in another notebook, which will help us download the models.",
   "id": "d60553a25db98abe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%run ../transformers/utils/model_retrieval.ipynb",
   "id": "24998ddf13fb8baf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And now we are ready to download our models. Each of these calls will take some time, depending on your internet connection.",
   "id": "c1e37b46252a902f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "load_huggingface_model(ai_lab_config, NAMED_ENTITY_MODEL, 'token-classification')",
   "id": "af6afb4d64b58dbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "load_huggingface_model(ai_lab_config, NLI_MODEL, 'zero-shot-classification')",
   "id": "ece30bce68be24d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "load_huggingface_model(ai_lab_config, FEATURE_EXTRACTION_MODEL, 'feature-extraction')",
   "id": "c512a765d092723a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Further setup\n",
   "id": "695232dbe8a5558c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from exasol.ai.text.extraction import *\n",
    "from exasol.ai.text.extraction.extraction import Extraction\n",
    "from exasol.ai.text.extraction.abstract_extraction import Output"
   ],
   "id": "a787606978778e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "schema=ai_lab_config.db_schema\n",
    "table=\"CUSTOMER_SUPPORT_TICKETS\"\n",
    "text_column=\"TICKET_DESCRIPTION\"\n",
    "key_column=\"TICKET_ID\"\n",
    "topics=[\"hardware issue\", \"software issue\"]"
   ],
   "id": "6693fbd5f13d6b9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define which steps to run\n",
    "\n"
   ],
   "id": "77e177878abf872c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%config SqlMagic.displaylimit = 20",
   "id": "4f0ca7c6815c1db1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%run ./utils/txaie_default_extractor.ipynb",
   "id": "881b49efcdbf7e91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%run ./utils/txaie_extaction_wrapper.ipynb",
   "id": "46dddab3fc181d5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " here we explain what the extraction wrapper and default extraction do and where to find them",
   "id": "85d7cc7029cc4d60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#todo put stuff into secret store?\n",
    "extraction = ExtractionWrapper()\n"
   ],
   "id": "f9ebd37ab1ed766"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_text_ai_preprocessing():\n",
    "    with open_pyexasol_connection(ai_lab_config, compression=True) as conn:\n",
    "        conn.execute(query=activation_sql)\n",
    "        extraction.run(conn, schema, \"PYTHON3_TXAIE\")"
   ],
   "id": "7f748392e800f4ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Run the preprocessing\n",
    "\n",
    "    with half data\n",
    "\n",
    "    first, show number of rows in data table"
   ],
   "id": "5d9edd47bd725c64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "run_text_ai_preprocessing()",
   "id": "4ce1e6f25211fe06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The next call will make it possible to run sql directly in this notebook, in order to easyer display the results of out preprocessing.",
   "id": "fee8734bdcd5210f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%run ../../utils/jupysql_init.ipynb",
   "id": "c1dbab964ee1b02f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%config SqlMagic.displaylimit = 10",
   "id": "6f54b13d5f0ffd3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First, lets look at which tables where created by our preprocessing:\n",
    "\n",
    "    talk about what they contain?"
   ],
   "id": "62bd86aecabf8df2"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql1"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "SELECT TABLE_SCHEMA, TABLE_NAME FROM EXA_ALL_TABLES"
   ],
   "id": "4ad2049e6a00cd28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There are also some new views:",
   "id": "f4255e3dc376f9fb"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql2"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "SELECT VIEW_SCHEMA, VIEW_NAME FROM EXA_ALL_VIEWS"
   ],
   "id": "6530d44adfd951a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    show example result rows,",
   "id": "2cf7ba49e2779850"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "429842a118fff88c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    Show Tables counts for Documents, Extractions and Audit Log",
   "id": "5b9fe941834f300f"
  },
  {
   "metadata": {
    "SqlCellData": {
     "variableName$1": "df_sql3"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%sql\n",
    "SELECT COUNT(ALL text_doc_id) FROM {{schema}}.DOCUMENTS; # or TEST_TXAI_DOCUMENTS?\n"
   ],
   "id": "e7b0f720fc89114e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Change config and a second run\n",
    "\n",
    "    # todo how to change conifg?\n"
   ],
   "id": "b1350f457583a5ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "run_text_ai_preprocessing()",
   "id": "6df4444eb7688a2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    Show Tables counts for Documents, Extractions and Audit Log\n",
   "id": "48e1f4144f54a9f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Adding data to source and a third run\n",
    "\n",
    "add second data half to first data table, run again"
   ],
   "id": "aca5b4450cbc9e70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "run_text_ai_preprocessing()",
   "id": "9ac78916bcd213be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    talk about time preprocessing takes in step 1 and step 3, compare, discuss how is only run on new data.",
   "id": "40e6b3589cbd714e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "    Show Tables counts for Documents, Extractions and Audit Log",
   "id": "c13cb16eec739f01"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
