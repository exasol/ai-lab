{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e31a8e-3da8-416f-9fc5-13d1c23664c1",
   "metadata": {},
   "source": [
    "In this notebook we are going to create a universal UDF SET script that will use a trained scikit-learn model for making a prediction. In order to use this script one has to create and train a scikit-learn model or a pipeline and upload its pickle file into the BucketFS. The list of features supplied to the UDF should match those used in model training and/or pre-processing. The script emits the prediction labels. The output of the script can be multi-dimensional. It works similarly in regression and classification scenarios.\n",
    "\n",
    "To communicate with Exasol database we will be using the <a href=\"https://github.com/exasol/pyexasol\" target=\"_blank\" rel=\"noopener\">`pyexasol`</a> module.\n",
    "\n",
    "Prior to using this notebook one needs to [create the database schema](setup_db.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0da0339-bedf-4433-be03-921650f3837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move this to a separate configuration notebook. Here we just need to load this configuration from a store.\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SandboxConfig:\n",
    "    EXTERNAL_HOST_NAME = \"192.168.124.93\"\n",
    "    HOST_PORT = \"8888\"\n",
    "\n",
    "    @property\n",
    "    def EXTERNAL_HOST(self):\n",
    "        return f\"\"\"{self.EXTERNAL_HOST_NAME}:{self.HOST_PORT}\"\"\"\n",
    "\n",
    "    USER = \"sys\"\n",
    "    PASSWORD = \"exasol\"\n",
    "    BUCKETFS_PORT = \"6666\"\n",
    "    BUCKETFS_USER = \"w\"\n",
    "    BUCKETFS_PASSWORD = \"write\"\n",
    "    BUCKETFS_USE_HTTPS = False\n",
    "    BUCKETFS_SERVICE = \"bfsdefault\"\n",
    "    BUCKETFS_BUCKET = \"default\"\n",
    "\n",
    "    @property\n",
    "    def EXTERNAL_BUCKETFS_HOST(self):\n",
    "        return f\"\"\"{self.EXTERNAL_HOST_NAME}:{self.BUCKETFS_PORT}\"\"\"\n",
    "\n",
    "    @property\n",
    "    def BUCKETFS_URL_PREFIX(self):\n",
    "        return \"https://\" if self.BUCKETFS_USE_HTTPS else \"http://\"\n",
    "\n",
    "    @property\n",
    "    def BUCKETFS_PATH(self):\n",
    "        # Filesystem-Path to the read-only mounted BucketFS inside the running UDF Container\n",
    "        return f\"/buckets/{self.BUCKETFS_SERVICE}/{self.BUCKETFS_BUCKET}\"\n",
    "\n",
    "    SCRIPT_LANGUAGE_NAME = \"PYTHON3_60\"\n",
    "    UDF_FLAVOR = \"python3-ds-EXASOL-6.0.0\"\n",
    "    UDF_RELEASE= \"20190116\"\n",
    "    UDF_CLIENT = \"exaudfclient\" # or for newer versions of the flavor exaudfclient_py3\n",
    "    SCHEMA = \"IDA\"\n",
    "\n",
    "    @property\n",
    "    def SCRIPT_LANGUAGES(self):\n",
    "        return f\"\"\"{self.SCRIPT_LANGUAGE_NAME}=localzmq+protobuf:///{self.BUCKETFS_SERVICE}/\n",
    "            {self.BUCKETFS_BUCKET}/{self.UDF_FLAVOR}?lang=python#buckets/{self.BUCKETFS_SERVICE}/\n",
    "            {self.BUCKETFS_BUCKET}/{self.UDF_FLAVOR}/exaudf/{self.UDF_CLIENT}\"\"\";\n",
    "\n",
    "    @property\n",
    "    def connection_params(self):\n",
    "        return {\"dns\": self.EXTERNAL_HOST, \"user\": self.USER, \"password\": self.PASSWORD, \"compression\": True}\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return {\n",
    "            \"script_languages\": self.SCRIPT_LANGUAGES,\n",
    "            \"script_language_name\": self.SCRIPT_LANGUAGE_NAME,\n",
    "            \"schema\": self.SCHEMA,\n",
    "            \"BUCKETFS_PORT\": self.BUCKETFS_PORT,\n",
    "            \"BUCKETFS_USER\": self.BUCKETFS_USER,\n",
    "            \"BUCKETFS_PASSWORD\": self.BUCKETFS_PASSWORD,\n",
    "            \"BUCKETFS_USE_HTTPS\": self.BUCKETFS_USE_HTTPS,\n",
    "            \"BUCKETFS_BUCKET\": self.BUCKETFS_BUCKET,\n",
    "            \"BUCKETFS_PATH\": self.BUCKETFS_PATH\n",
    "        }\n",
    "\n",
    "conf = SandboxConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c74dbb1-e40b-427e-9140-fc33ded4798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating prediction script took: 106.19ms\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import pyexasol\n",
    "from stopwatch import Stopwatch\n",
    "\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Create script to test the model\n",
    "sql = textwrap.dedent(\"\"\"\\\n",
    "CREATE OR REPLACE PYTHON3 SET SCRIPT\n",
    "{schema!i}.SKLEARN_PREDICT(...)\n",
    "EMITS(...) AS\n",
    "\n",
    "# Generic scikit-learn predictor that runs a prediction for a data batch.\n",
    "# Loads a scikit-learn model or a pipeline from the specified file. Calls its `predict` method\n",
    "# passing to it all provided data columns. Emits sample IDs and the output of the model.\n",
    "#\n",
    "# Note that the model should not include features' names!\n",
    "# \n",
    "# Input columns:\n",
    "#    [0]:  Full BucketFS path to the model file;\n",
    "#    [1]:  Sample ID, can be the ROWID of the test batch.\n",
    "#    [2+]: Feature columns.\n",
    "#\n",
    "# Output columns:\n",
    "#    [0]:  Sample ID copied from the input.\n",
    "#    [1+]: Model output.\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def run(ctx):\n",
    "    # Load model from EXABucket\n",
    "    with open(ctx[0], 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "    # Stream the data through the model to reduce the required main memory of the UDF.\n",
    "    # This allows running the UDF on larger datasets.\n",
    "    while True:\n",
    "        # Read the input skipping the first column which holds the model path.\n",
    "        X_pred = ctx.get_dataframe(num_rows=1000, start_col=1)\n",
    "        if X_pred is None:\n",
    "            break\n",
    "\n",
    "        # Call the model to get the predictions. Omit the first column in the input\n",
    "        # which holds the sample ids.\n",
    "        df_features = X_pred.drop(X_pred.columns[0], axis=1)\n",
    "        y_pred = model.predict(df_features)\n",
    "\n",
    "        # Combine predictions with the sample ids.\n",
    "        df_rowid = X_pred[X_pred.columns[0]].reset_index(drop=True)\n",
    "        df_pred = pd.concat((df_rowid, pd.DataFrame(y_pred)), axis=1)\n",
    "\n",
    "        # Output data\n",
    "        ctx.emit(df_pred)\n",
    "/\n",
    "\"\"\")\n",
    "\n",
    "with pyexasol.connect(dsn=conf.EXTERNAL_HOST, user=conf.USER, password=conf.PASSWORD, compression=True) as conn:\n",
    "    conn.execute(query=sql, query_params=conf.params)\n",
    "\n",
    "print(f\"Creating prediction script took: {stopwatch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eac55c-52c1-41e1-8df4-41958d994bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
