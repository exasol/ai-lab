{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "289d2a8c-953d-46e5-8c73-ad810c29b20f",
   "metadata": {},
   "source": [
    "In this notebook we will train a simple regression model for predicting the age of an abalone from its physical measurements and sex.You can find more information about the problem domain <a href=\"https://archive.ics.uci.edu/dataset/1/abalone\" target=\"_blank\" rel=\"noopener\">here</a>.\n",
    "\n",
    "We will train the model in this notebook using <a href=\"https://scikit-learn.org/stable/\" target=\"_blank\" rel=\"noopener\">`scikit-learn`</a>, on the training data we are going to export from the database.\n",
    "\n",
    "To execute queries and load data from Exasol database we will be using the <a href=\"https://github.com/exasol/pyexasol\" target=\"_blank\" rel=\"noopener\">`pyexasol`</a> module.\n",
    "\n",
    "Prior to using this notebook one needs to complete the follow steps:\n",
    "1. [Create the database schema](../setup_db.ipynb).\n",
    "2. [Load the Abalone data](../data/data_abalone.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa6c628f-853e-4850-8bab-46f7f645856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move this to a separate configuration notebook. Here we just need to load this configuration from a store.\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SandboxConfig:\n",
    "    EXTERNAL_HOST_NAME = \"192.168.124.93\"\n",
    "    HOST_PORT = \"8888\"\n",
    "\n",
    "    @property\n",
    "    def EXTERNAL_HOST(self):\n",
    "        return f\"\"\"{self.EXTERNAL_HOST_NAME}:{self.HOST_PORT}\"\"\"\n",
    "\n",
    "    USER = \"sys\"\n",
    "    PASSWORD = \"exasol\"\n",
    "    BUCKETFS_PORT = \"6666\"\n",
    "    BUCKETFS_USER = \"w\"\n",
    "    BUCKETFS_PASSWORD = \"write\"\n",
    "    BUCKETFS_USE_HTTPS = False\n",
    "    BUCKETFS_SERVICE = \"bfsdefault\"\n",
    "    BUCKETFS_BUCKET = \"default\"\n",
    "\n",
    "    @property\n",
    "    def EXTERNAL_BUCKETFS_HOST(self):\n",
    "        return f\"\"\"{self.EXTERNAL_HOST_NAME}:{self.BUCKETFS_PORT}\"\"\"\n",
    "\n",
    "    @property\n",
    "    def BUCKETFS_URL_PREFIX(self):\n",
    "        return \"https://\" if self.BUCKETFS_USE_HTTPS else \"http://\"\n",
    "\n",
    "    @property\n",
    "    def BUCKETFS_PATH(self):\n",
    "        # Filesystem-Path to the read-only mounted BucketFS inside the running UDF Container\n",
    "        return f\"/buckets/{self.BUCKETFS_SERVICE}/{self.BUCKETFS_BUCKET}\"\n",
    "\n",
    "    SCRIPT_LANGUAGE_NAME = \"PYTHON3_60\"\n",
    "    UDF_FLAVOR = \"python3-ds-EXASOL-6.0.0\"\n",
    "    UDF_RELEASE= \"20190116\"\n",
    "    UDF_CLIENT = \"exaudfclient\" # or for newer versions of the flavor exaudfclient_py3\n",
    "    SCHEMA = \"IDA\"\n",
    "\n",
    "    @property\n",
    "    def SCRIPT_LANGUAGES(self):\n",
    "        return f\"\"\"{self.SCRIPT_LANGUAGE_NAME}=localzmq+protobuf:///{self.BUCKETFS_SERVICE}/\n",
    "            {self.BUCKETFS_BUCKET}/{self.UDF_FLAVOR}?lang=python#buckets/{self.BUCKETFS_SERVICE}/\n",
    "            {self.BUCKETFS_BUCKET}/{self.UDF_FLAVOR}/exaudf/{self.UDF_CLIENT}\"\"\";\n",
    "\n",
    "    @property\n",
    "    def connection_params(self):\n",
    "        return {\"dns\": self.EXTERNAL_HOST, \"user\": self.USER, \"password\": self.PASSWORD, \"compression\": True}\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return {\n",
    "            \"script_languages\": self.SCRIPT_LANGUAGES,\n",
    "            \"script_language_name\": self.SCRIPT_LANGUAGE_NAME,\n",
    "            \"schema\": self.SCHEMA,\n",
    "            \"BUCKETFS_PORT\": self.BUCKETFS_PORT,\n",
    "            \"BUCKETFS_USER\": self.BUCKETFS_USER,\n",
    "            \"BUCKETFS_PASSWORD\": self.BUCKETFS_PASSWORD,\n",
    "            \"BUCKETFS_USE_HTTPS\": self.BUCKETFS_USE_HTTPS,\n",
    "            \"BUCKETFS_BUCKET\": self.BUCKETFS_BUCKET,\n",
    "            \"BUCKETFS_PATH\": self.BUCKETFS_PATH\n",
    "        }\n",
    "\n",
    "conf = SandboxConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089a9b5-89b1-45a9-925d-521945ae89b7",
   "metadata": {},
   "source": [
    "First we will export data into a pandas DataFrame and split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68be5d5a-ebcc-4fb4-829f-29a5442579d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data took: 1.27s\n"
     ]
    }
   ],
   "source": [
    "import pyexasol\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stopwatch import Stopwatch\n",
    "\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "with pyexasol.connect(dsn=conf.EXTERNAL_HOST, user=conf.USER, password=conf.PASSWORD, compression=True) as conn:\n",
    "    df = conn.export_to_pandas(query_or_table=(conf.SCHEMA, 'ABALONE_TRAIN'))\n",
    "\n",
    "X, y = df.drop(columns='RINGS'), df['RINGS']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(f\"Loading the data took: {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4e36c7-06aa-4c2f-bf46-10b328cee623",
   "metadata": {},
   "source": [
    "Let's look at the features. We will first check what physical measurements have predictive power. For that we will compute mutual information between the measurements' input columns and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc3e231-b8fe-4498-9393-f65ecbac618e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LENGTH': 0.3777307983714149,\n",
       " 'DIAMETER': 0.4130225409481696,\n",
       " 'HEIGHT': 0.3730705618531607,\n",
       " 'WHOLE_WEIGHT': 0.3839492388004473,\n",
       " 'SHUCKED_WEIGHT': 0.33404766440572065,\n",
       " 'VISCERA_WEIGHT': 0.35472141609994523,\n",
       " 'SHELL_WEIGHT': 0.4472509397786135}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "X_meas = X_train.drop(columns='SEX')\n",
    "mi = mutual_info_regression(X_meas, y_train)\n",
    "dict(zip(X_meas.columns, mi))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb025ff-64e3-4118-8313-039da78f6a3c",
   "metadata": {},
   "source": [
    "Now let's see if SEX is a good predictor. We will do the ANOVA test and print the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "165e9d98-888e-473c-8d7a-1f304364b5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5467010907188578e-126"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "f_classif(y_train.to_frame(), X_train['SEX'])[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3cac4-8b4d-4423-8e7b-2ded0e8c77e8",
   "metadata": {},
   "source": [
    "Let's make a pipeline. We will use all features in the input. We will do One Hot Encoding of the SEX column and normalize all others, including the target. Let's use the Support Vector Machine as the regression model. We will drop the column names in the inputs, as they will not be available in the prediction UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4feb0d95-81f8-4b93-8012-a3f39ac37f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of the model took: 168.23ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Create the pipeline.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), [1, 2, 3, 4, 5, 6, 7]),\n",
    "        (\"cat\", OneHotEncoder(), [0]),\n",
    "    ]\n",
    ")\n",
    "regressor = SVR(kernel='rbf')\n",
    "model = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), ('regressor', regressor)]\n",
    ")\n",
    "model = TransformedTargetRegressor(regressor=model, transformer=StandardScaler())\n",
    "\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Train the model.\n",
    "model.fit(X_train.values, y_train.values)\n",
    "\n",
    "print(f\"Training of the model took: {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b81bf25-ef8c-4e2d-9c1b-1ef9e1cbed20",
   "metadata": {},
   "source": [
    "Let's see what prediction performance we've got, printing some regression metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "061744b4-c401-4b79-914f-d3d3ba3aad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 1.3993917946085437\n",
      "Mean squared error: 4.266767837896115\n",
      "Explained variance: 0.5731816112246741\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "y_pred = model.predict(X_valid.values)\n",
    "\n",
    "print('Mean absolute error:', mean_absolute_error(y_valid, y_pred))\n",
    "print('Mean squared error:', mean_squared_error(y_valid, y_pred))\n",
    "print('Explained variance:', explained_variance_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6fd07e-5175-4767-a5d0-339d42afa135",
   "metadata": {},
   "source": [
    "Now, let's upload the model into the BucketFS, so that it can be used for making predictions in SQL queries. To communicate with BucketFS we will be using the <a href=\"https://exasol.github.io/bucketfs-python/\" target=\"_blank\" rel=\"noopener\">`bucketfs-python`</a> module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f61810f9-3d9e-4e1a-aca3-81cf976540d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading the model took: 605.55ms\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from exasol.bucketfs import Service\n",
    "\n",
    "MODEL_FILE = 'abalone_svm_model.pkl'\n",
    "\n",
    "# Setup the connection parameters.\n",
    "buckfs_url = f'{conf.BUCKETFS_URL_PREFIX}{conf.EXTERNAL_BUCKETFS_HOST}'\n",
    "buckfs_credentials = {conf.BUCKETFS_BUCKET: {'username': conf.BUCKETFS_USER, 'password': conf.BUCKETFS_PASSWORD}}\n",
    "\n",
    "stopwatch = Stopwatch()\n",
    "\n",
    "# Connect to the BucketFS service and navigate to the bucket of choice.\n",
    "bucketfs = Service(buckfs_url, buckfs_credentials)\n",
    "bucket = bucketfs[conf.BUCKETFS_BUCKET]\n",
    "\n",
    "# Serialize model into a byte-array and upload it to the BucketFS, \n",
    "# where it will be saved in the file with the specified name.\n",
    "bucket.upload(MODEL_FILE, pickle.dumps(model))\n",
    "\n",
    "print(f\"Uploading the model took: {stopwatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edb448-40b9-4e6a-9d32-4a499334c467",
   "metadata": {},
   "source": [
    "Now we are ready to use this model in our SQL queries. This will be demonstrated in the [following notebook](sklearn_predict_abalone.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7be224-1cd9-436f-963b-0443f2003a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
